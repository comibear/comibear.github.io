---
title: "[Paper Review] Dense Passage Retrieval for Open-Domain Question Answering"
description: "Paper Review for DPR model"
writer: Sangyun Won
categories: [AI, Paper Review]
tags: [AI]
image:
  path: https://github.com/user-attachments/assets/544ee579-860b-4189-bdd6-8b3a3c626259
  alt: Paper Review for DPR model

math: true
toc: true
toc_sticky: true

date: 2024-09-18
last_modified_at: 2024-09-26
---

<style>
  figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}
.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}
</style>

<figure>
  <a href="https://arxiv.org/pdf/2004.04906" class="bookmark source">
    <div class="bookmark-info">
      <div class="bookmark-text">
        <div class="bookmark-title">Dense Passage Retrieval for Open-Domain Question Answering</div>
        <div class="bookmark-description">arxiv pdf link for DPR</div>
      </div>
    </div>
  </a>
</figure>

## 0. Abstract ğŸ¬

`Open-domain question answering` ì€ ë‹µë³€ì— íš¨ê³¼ì ì¸ passageë¥¼ `retrieval` í•˜ëŠ” ë°©ì‹ì— ì˜ì¡´í•œë‹¤. ì´ ë°©ì‹ì€ ì§€ê¸ˆê¹Œì§€ `TF-IDF`, `BM25`ì™€ ê°™ì€ `sparse vector space`ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì— ê¸°ì¸í•´ ì™”ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ `retrieval`ì„ ìœ„í•œ vector representationì´ ì ì€ ìˆ˜ì˜ `question & passage`ë¥¼ ì‚¬ìš©í•œ `dual-encoder framework`ë¥¼ ì´ìš©í•˜ì—¬, `dense representation`ìœ¼ë¡œë„ ì‚¬ì‹¤ìƒ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì¸ë‹¤.

ê´‘ë²”ìœ„í•œ `open-domain QA dataset`ì„ ì´ìš©í•˜ì—¬ í‰ê°€í–ˆì„ ë•Œ, ë³¸ ë…¼ë¬¸ì˜ `dense retriever`ì€ ìƒìœ„ 20ê°œì˜ `retrieval` ì •í™•ë„ì—ì„œ ê¸°ì¡´ì˜ `Lucene-BM25` ì‹œìŠ¤í…œ ì ˆëŒ€ì ìœ¼ë¡œ 9% ~ 19% ëŠ¥ê°€í–ˆë‹¤. ë˜í•œ, ì´ëŠ” `open-domain QA` ì—ì„œ ìƒˆë¡œìš´ `SOTA`ë¥¼ ë‹¬ì„±í–ˆë‹¤.

## 1. Introduction â˜•ï¸

`QA (Open-domain Question Answering)` ì€ ê±°ëŒ€í•œ ë¬¸ì„œì˜ ì§‘í•©ì„ ì‚¬ìš©í•˜ì—¬ `factoid question`ì— ë‹µë³€í•˜ëŠ” taskì´ë‹¤. ì´ì „ì˜ `QA system`ì€ ë‹¤ì†Œ ë³µì¡í•˜ê³ , ë‹¤ì–‘í•œ ìš”ì†Œë¡œ ì´ë£¨ì–´ì ¸ ìˆì—ˆë‹¤. í•˜ì§€ë§Œ `Comprehension model`ì˜ ë°œë‹¬ë¡œ ì¸í•´, ë§¤ìš° ê°„ë‹¨í•œ `two-stage framework`ë¡œ ë‚˜ëˆ„ì–´ì¡Œë‹¤.

1. **context retrieverì´ ë¨¼ì € ë‹µë³€ì„ ìœ„í•œ passageë“¤ì˜ ì‘ì€ ì§‘í•©ì„ ì„ íƒí•œë‹¤.**

2. **ê·¸ë¦¬ê³  readerì´ retrieved ëœ contextë“¤ì„ ë¶„ì„í•˜ì—¬ ì˜¬ë°”ë¥¸ ì •ë‹µì„ ë„ì¶œí•œë‹¤.**

ë¬¼ë¡  QA taskë¥¼ machine readingë§Œì˜ taskë¡œ ë°”ë¼ë³´ëŠ” ê´€ì  ë˜í•œ ì¶©ë¶„íˆ ê³ ë ¤í•  ë§Œ í•˜ì§€ë§Œ, huge performance degradationì˜ ì‚¬ë¡€ê°€ ì¡´ì¬í•˜ê¸°ì—, retrievalì— ëŒ€í•œ í–¥ìƒì´ í•„ìš”í•˜ë‹¤ê³  ì—¬ê²¨ì§„ë‹¤.

QAì—ì„œ retrievalì€ ì£¼ë¡œ TF-IDF ì´ë‚˜ BM25ë¡œ êµ¬í˜„ë˜ì–´ ì™”ëŠ”ë°, ì´ëŠ” keyword ë¥¼ ì¤‘ì ìœ¼ë¡œ sparse vectorë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•ì´ì˜€ë‹¤. ë°˜ëŒ€ë¡œ, dense vectorì„ latent semantic encoding ì„ í™œìš©í•˜ì—¬ ì•ì„  sparse vectorê³¼ëŠ” ìƒë³´ì ì¸ ê´€ê³„ì— ìˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì™€ ê°™ì€ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ì.

> _<span style="color:red;">**Q : Who is the bad guy in lord of the rings?**</span>_ <br><br>_<span style="color:blue;">**Useful context : Sala Baker is best known for portraying the villain Sauron in the Lord of the Rings trilogy.**</span>_

`Term-based system`ì€ _villain_ ê³¼ *bad guy*ì— ëŒ€í•œ `semantic similarity`ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•Šê¸° ë–„ë¬¸ì—, í•´ë‹¹ `context`ë¥¼ `retrieval` í•˜ê¸° ì–´ë µì§€ë§Œ, `dense retrieval system`ì€ ì´ ë‘ ë‹¨ì–´ë¥¼ ì—°ê²°ì§€ì–´ í•´ë‹¹ `context`ë¥¼ `reteival` í•  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.

ë” ë‚˜ì•„ê°€ì„œ `Dense encoding` ì€ `learable` í•˜ê¸° ë•Œë¬¸ì—, íŠ¹ì • taskì— ëŒ€í•´ specific í•˜ê²Œ í•™ìŠµí•˜ì—¬ ìœ ì—°ì„± ë˜í•œ ê°€ì§€ê³  ìˆë‹¤. ì´ëŸ° ê³¼ì •ì€ `MIPS (maximum inner product search) Algorithm`ì„ í†µí•´ì„œ ê³„ì‚°ëœë‹¤.

ê·¸ëŸ¬ë‚˜, ì¼ë°˜ì ìœ¼ë¡œ ì¢‹ì€ `dense vector representation`ì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ í° ìˆ˜ì˜ `question & context pair` ì´ í•„ìš”í•˜ë‹¤ê³  ì—¬ê²¨ì ¸ ì™”ë‹¤. `Dense retrieval` ë°©ë²•ì€ `TF-IDF`/`BM25`ì™€ ê°™ì€ ê³ ì „ ë°©ì‹ì„ ëŠ¥ê°€í•˜ì§€ ëª»í–ˆì—ˆì§€ë§Œ, `ICT (inverse cloze task) training`ì„ ì´ìš©í•œ ëª¨ë¸ì¸ `ORQA`ê°€ ì²˜ìŒìœ¼ë¡œ ì´ ë°©ì‹ì„ ëŠ¥ê°€í•˜ê²Œ ë˜ì—ˆë‹¤.

> ì—¬ê¸°ì„œ `ICT (inverse cloze task)` ë€, `context` ë‚´ì—ì„œ íŠ¹ì • `sentence`ë¥¼ ì¶”ì¶œí•˜ì—¬, í•´ë‹¹ `sentence` ê°€ ì–´ëŠ `context`ì— ì†í•˜ëŠ”ì§€ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤.

í•˜ì§€ë§Œ, ì•ì„  `ORQA`ì˜ ì„±ëŠ¥ì—ë„ ë¶ˆêµ¬í•˜ê³ , `multiple domain` ìƒì—ì„œì˜ SOTAë¥¼ ë‹¬ì„±í•˜ê¸°ì—ëŠ” 2ê°€ì§€ ë¶€ë¶„ì—ì„œ ë¬¸ì œì ì´ ìˆì—ˆë‹¤.

1. **ICTëŠ” computationally intensive í•˜ê³ , ë‹¨ìˆœíˆ sentenceë¥¼ matching ì‹œí‚¤ëŠ” ê²ƒì´ Question anwseringì— íš¨ê³¼ì ì´ë¼ëŠ” ê²ƒì´ ëª…í™•í•˜ì§€ ì•Šë‹¤.**
2. **context encoderì€ question-answer ìŒì„ ì´ìš©í•´ fine-tuned ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì—, í•´ë‹¹ encoderì„ í†µí•œ representationì´ ìµœì ì˜ ê°’ì´ ì•„ë‹ ìˆ˜ ìˆë‹¤.**

ë³¸ ë…¼ë¬¸ì—ì„œëŠ”, ì¶”ê°€ì ì¸ `pre-training` ì—†ì´ `question-answer` ì˜ ìŒë“¤(Not so much)ë§Œ ì´ìš©í•˜ì—¬ ë” ë‚˜ì€ `dense embedding model` ì„ ë§Œë“œëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. `Pretrained BERT model`ê³¼ `dual-encoder`ì„ í™œìš©í•˜ì—¬, ìƒëŒ€ì ìœ¼ë¡œ ì ì€ ìˆ˜ì˜ question-passage(answer) ìŒì„ ì´ìš©í•˜ë„ë¡ í•  ê²ƒì´ë‹¤.

ì´ ê³¼ì •ì—ì„œ, ìœ ì‚¬í•œ question-passage ë“¤ì˜ ë‚´ì ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ìµœì í™”ë¥¼ ì§„í–‰í•  ê²ƒì´ë©°, batch ë‚´ì˜ ëª¨ë“  question, passage ìŒì„ ë¹„êµí•  ê²ƒì´ë‹¤. ë³¸ ë…¼ë¬¸ì˜ DPR methodì€ `BM25` ë°©ì‹ì„ í° ì°¨ì´ë¡œ ëŠ¥ê°€í•˜ë©°, ë‹¨ìˆœíˆ representationì´ ì•„ë‹Œ `end-to-end QA` ì •í™•ë„ ë˜í•œ `ORQA`ì— ë¹„í•´ í° ì°¨ì´ë¥¼ ëƒˆë‹¤.

ë³¸ ë…¼ë¬¸ì€ ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ question-answer ì˜ ìŒë“¤ì„ í•™ìŠµí•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„, `BM25`ì˜ ì„±ëŠ¥ì„ ëŠ¥ê°€í•œë‹¤. ë˜í•œ, ì´ëŠ” ì¶”ê°€ì ì¸ `pre-train`ì„ ìš”êµ¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ë˜í•œ `Open-domain` ì—ì„œ retrievalì˜ ì„±ëŠ¥ì´ ë†’ì„ìˆ˜ë¡, `end-to-end` ì˜ QA ì„±ëŠ¥ ë˜í•œ ë†’ì•„ì§„ë‹¤.

## 2. Background ğŸ§

`open-domain` ì˜ ì£¼ìš” taskëŠ” ë‹¤ìŒê³¼ ê°™ì€ `factoid question`ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ë‹¤ì–‘í•œ ì£¼ì œì˜ topic ì— ëŒ€í•œ corpusë¥¼ ì°¸ì¡°í•˜ì—¬ ì •ë‹µì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤.

> Q : Who first voiced Meg on Family Guy? <br> Q: Where was the 8th Dalai Lama born?

ë” êµ¬ì²´ì ìœ¼ë¡œëŠ”, QA ë¥¼ extractive í•œ question ì— í•œì •ì§“ëŠ”ë‹¤. ë‹¤ì‹œ ë§í•´ questionì— ëŒ€í•œ ì •ë‹µì€ í•­ìƒ corpus setì˜ document ì— í•˜ë‚˜ ì´ìƒ ì¡´ì¬í•œë‹¤ê³  ê°€ì •í•œë‹¤.

í•˜ì§€ë§Œ ì´ëŸ° ë©”ì»¤ë‹ˆì¦˜ì˜ ê²½ìš°, `open-domain question`ì˜ íŠ¹ì„±ìƒ ë§¤ìš° ë§ì€ documentê°€ ì¡´ì¬í•´ì•¼ í•˜ë©°, corpusì˜ í¬ê¸°ëŠ” millions of document ì—ì„œ billionê¹Œì§€ ë§¤ìš° í° ìˆ˜ëŸ‰ì„ ê°€ì§„ë‹¤ë‹¤.

ë”°ë¼ì„œ ì´ë¥¼ ìœ„í•œ `efficient retriever component`, ì¦‰ ì •í™•í•œ ì •ë‹µì„ ì°¾ì•„ë‚´ê¸° ì „ì— queryì™€ ìœ ì‚¬í•œ ì§‘í•© (ì „ì²´ corpusì˜ ë¶€ë¶„ì§‘í•©)ì„ êµ¬í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ í•„ìš”í•˜ë‹¤. Retriever $R$ ì„ $R : (q, \mathcal{C}) \rightarrow \mathcal{C}\_{\mathcal{F}}$
, $\mathcal{C}$ë¥¼ corpus, $q$ ë¥¼ question ì´ë¼ê³  í–ˆì„ ë•Œ, retrieverì€ $\mathcal{C}\_{\mathcal{F}} \in \mathcal{C},\;\; |\mathcal{C}\_{\mathcal{F}}| = k \ll |\mathcal{C}|$ í•œ corpus $\mathcal{C}$ì˜ subsetì¸ $\mathcal{C}\_{\mathcal{F}}$ë¥¼ êµ¬í•´ë‚´ì–´ì•¼ í•œë‹¤.

## 3. Dense Passage Retriever (DPR) ğŸ¥½

ì´ ë…¼ë¬¸ì—ì„œëŠ” `open-domain QA task`ì—ì„œ `retrieval component` ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ë°ì— ì¤‘ì ì„ ë‘”ë‹¤. $M$ ê°œì˜ text passage ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, `DPR` ì˜ ëª©í‘œëŠ” ì´ ëª¨ë“  passageë¥¼ ëª¨ë‘ low-dimensionalë¡œ ë³€í™˜ì‹œì¼œ `top-k relevant passage` ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ íš¨ê³¼ì ìœ¼ë¡œ retrieval í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤. (ë‹¹ì—°í•˜ê²Œë„ $M$ì€ ë§¤ìš° í° ì§‘í•©ìœ¼ë¡œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” 21 million ì •ë„ì´ë‹¤.)

### 3.1 Overview

ë³¸ ë…¼ë¬¸ì˜ `DPR`ì€ text passageë¥¼ $d$-dimensional `real-valued vector`ë¡œ encoding í•˜ëŠ” `dense encoder` $E_p()$ ë¥¼ ì‚¬ìš©í•œë‹¤. run-time ë•ŒëŠ”, ì´ì™€ ë‹¤ë¥¸ encoderì¸ $E_Q()$ ê°€ ì‚¬ìš©ë˜ëŠ”ë°, ì´ëŠ” input question ì„ $d$-dimensional vectorë¡œ ë³€í™˜í•˜ì—¬, ì´ ë²¡í„°ë“¤ê°„ì˜ ê³„ì‚°ì„ í†µí•´ `top-k relevant passage` ë¥¼ retrieval í•˜ê²Œ ëœë‹¤. ì´ relevantness ê³„ì‚°ì€ ë‹¤ìŒê³¼ ê°™ì€ `dot product`ë¡œ ê³„ì‚°ëœë‹¤.

$$ sim(q, p) = E_Q(q) \cdot E_P(p)$$

ë¬¼ë¡  `Cross Attention` ê³¼ ê°™ì´ ë‘ context ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ë”ìš± ì •í™•í•˜ê²Œ ì¸¡ì •í•  ìˆ˜ ìˆëŠ” ë°©ì‹ ë˜í•œ ì¡´ì¬í•˜ì§€ë§Œ, ë§¤ìš° í° ìˆ˜ëŸ‰ì˜ passage ë“¤ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œëŠ” `decomposable` í•œ ë°©ì‹ì´ ë”ìš± íš¨ìœ¨ì ì´ë‹¤. ëŒ€ë¶€ë¶„ì˜ `decomposable` í•œ `similarity function` ì€ `Euclidean distance (L2)` ì´ë©°, `cosine similarity` ë˜í•œ unit vector ë“¤ì— ëŒ€í•œ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê¸° ìš©ì´í•˜ì§€ë§Œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” `L2` ì™€ `cosine similarity`ë¥¼ ì´ì–´ì£¼ë©°, ë”ìš± ê°„ë‹¨í•œ `dot product` ë¥¼ ì‚¬ìš©í•œë‹¤.

#### Encoders

question ê³¼ passage ë¥¼ encoding í•˜ëŠ” ë°©ì‹ì—ëŠ” neural network ë„ ìˆì§€ë§Œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” 2ê°œì˜ ë…ë¦½ì ì¸ `BERT` ë¥¼ ì‚¬ìš©í•´ [CLS] í† í°ì„ representationìœ¼ë¡œ ì‚¬ìš©í•  ê²ƒì´ë‹¤.

#### Inference

Inference ì‹œê°„ì—ëŠ”, `passage encoder` $E_P$ë¥¼ ëª¨ë“  passage ì— ëŒ€í•´ì„œ ì ìš©í•˜ë©°, FAISSë¥¼ ì´ìš©í•˜ì—¬ indexing í•œë‹¤. `FAISS`ëŠ” `open-source library`ë¡œ, dense vectorë“¤ì— ëŒ€í•œ íš¨ìœ¨ì ì¸ `clustering`ì„ í†µí•´ì„œ ë§¤ìš° ë§ì€ ìˆ˜ì˜ vectorë“¤ì„ ë‹¤ë£° ìˆ˜ ìˆë‹¤. Question $p$ê°€ ì£¼ì–´ì§€ë©´, $v_q = E_Q(q)$ë¥¼ í†µí•´ top-$k$ passageë“¤ì„ ì°¾ëŠ”ë‹¤.

ì´ ê³¼ì •ì—ì„œ `HNSW` ë¼ëŠ” `ANN` ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ”ë°, [ë‹¤ìŒ ë§í¬](https://www.youtube.com/watch?v=hCqF4tDPNBw)ì— ìƒì„¸í•˜ê²Œ ì„¤ëª…ë˜ì–´ ìˆë‹¤. 

### 3.2 Training

Encoderë“¤ì„ í•™ìŠµì‹œì¼œ `dot-product similarity`ë¥¼ í™œìš©í•´ retrievalì„ íš¨ê³¼ì ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒì€ `metric learning problem`ì´ë‹¤. ì´ ëª©ì ì€ ê³§ `relevant question-passage` ìŒì— ëŒ€í•œ distanceë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì— ìˆë‹¤.

question $q$ ì— ëŒ€í•´ì„œ í•™ìŠµì„ ì§„í–‰í•  ë•Œ, $p_i^+$ë¥¼ `positive passage`
(relevant passage for $q$)ë¼ê³  í–ˆì„ ë•Œ, training data $\mathcal{D}$ ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

$$ \mathcal{D} = \{\left\langle q_i, p_i^+, p_{i,1}^-, \cdots, p_{i, n}^- \right\rangle\}_{i=1}^m $$

$\mathcal{D}$ëŠ” nê°œì˜ instanceë¥¼ ê°€ì§€ë©°, 1ìŒì˜ ì˜¬ë°”ë¥¸ `question-answer` ê³¼ question ì— ê´€ê³„ì—†ëŠ” `negative passage` ë¥¼ m ê°œ ê°€ì§€ê³  ìˆë‹¤. ìš°ë¦¬ëŠ” ì´ dataì— ëŒ€í•œ `loss function`ì„ `positive passage`ì— ëŒ€í•œ `negative log likelihood`ë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

$$L(q_i, p_i^+, p_{i,1}^-, \cdots, p_{i, n}^-) = -log\frac{e^{sim(q_i, p_i^+)}}{e^{sim(q_i, p_i^+)} + \sum_{j=1}^{n}{e^{sim(q_i, p_{i,j}^-)}}}$$

#### Positive and negative passages

ì´ í•™ìŠµì„ ìœ„í•´ì„œëŠ”, `question-passage`ì˜ ì ì ˆí•œ ìŒì„ ì°¾ê¸°ì—ëŠ” ëª…í™•í•˜ì§€ë§Œ, `negative passage`ë“¤ì„ ì°¾ê¸° ìœ„í•´ì„œëŠ” ë§¤ìš° í° í’€ì—ì„œ sampling ë˜ì–´ì•¼ í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, `positive passage`ëŠ” QA datasetë‚´ì— contextê°€ ì¡´ì¬í•˜ê±°ë‚˜ answerë¥¼ searching í•´ì„œ ì°¾ì„ ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  ë‹¤ë¥¸ ëª¨ë“  passage, ì¦‰ relevant í•˜ì§€ ì•Šì€ passageëŠ” `negative passage` ì´ë‹¤. ì‹¤ì œë¡œë„ ì–´ë– í•œ ë°©ì‹ìœ¼ë¡œ `negative passage`ë¥¼ êµ¬í•˜ëŠëƒëŠ” ì£¼ë¡œ ì¤‘ìš”í•˜ê²Œ ì—¬ê²¨ì§€ì§€ ì•Šì§€ë§Œ, ë•Œë¡œëŠ” ë†’ì€ ì„±ëŠ¥ì˜ encoderì„ êµ¬í˜„í•˜ëŠ” ë°ì— ì£¼ìš”í•œ ì—­í• ì„ í•œë‹¤.

ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” `negative passage`ë¥¼ êµ¬í•˜ëŠ” ë°©ì‹ì„ 3ê°€ì§€ ì¢…ë¥˜ë¡œ ë‚˜ëˆ„ì—ˆë‹¤.

1. <span style='color:blue'>**Random passage**</span> : corpus ë‚´ë¶€ì— ìˆëŠ” ëœë¤í•œ passage
2. <span style='color:blue'>**BM25**</span> : BM25ë¥¼ í†µí•´ ê³„ì‚°ë˜ì–´ answerì´ í¬í•¨ë˜ì–´ ìˆì§€ëŠ” ì•Šì§€ë§Œ, question ì˜ í† í°ì„ ê°€ì¥ ë§ì´ í¬í•¨ëœ passage
3. <span style='color:blue'>**Gold**</span> : training set ë‚´ë¶€ì˜ ë‹¤ë¥¸ questionì— ëŒ€í•´ì„œ positive passageë¡œ íŒë³„ëœ passage

ì´ëŸ¬í•œ ë°©ë²•ë“¤ì— ëŒ€í•´ì„œëŠ” [Section 5.2](#52-ablation-study-on-model-training) ì—ì„œ ë‹¤ë£° ê²ƒì´ë©°, ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë˜ ë°©ë²•ì€ `gold passage`ë“¤ì„ ê°™ì€ í¬ê¸°ì˜ mini-batchì— ëŒ€í•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì— 1ê°œì˜ `BM25` ê¸°ë°˜ `negative passage`ì„ ë”í•˜ëŠ” ê²ƒì´ë‹¤. ë” ë‚˜ì•„ê°€ì„œ, `gold` ê¸°ë°˜ì˜ `negative passage`ë¥¼ ì¬ì‚¬ìš©í•˜ëŠ” ê²ƒì€ `computational` ì ì¸ ê´€ì ì—ì„œ íš¨ìœ¨ì„±ì„ ë³´ì¸ë‹¤.

#### In-batch negatives

í•˜ë‚˜ì˜ mini-batchì— $B$ê°œì˜ questionì´ ì¡´ì¬í•œë‹¤ê³  í•˜ì. ê·¸ë¦¬ê³  respectively í•˜ê²Œ relevant í•œ passageì™€ ìŒì„ ì´ë£¨ê³  ìˆë‹¤. ì´ questions ë“¤ê³¼ passages ë“¤ì„ vectorë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì„ í–‰ë ¬í™”í•œ ê²ƒì„ ê°ê° $Q$, $P$ ë¼ê³  í•˜ë©´ (ê°ê°ì˜ í¬ê¸°ëŠ” $(B\times d)$ê°€ ëœë‹¤.), $S=QP^T$ ëŠ” ê³§ ê° questionê³¼ passage ê°„ì˜ ë…ë¦½ì ì¸ `similarity`ë¥¼ ë‚˜íƒ€ë‚´ê²Œ ëœë‹¤.

ì´ë ‡ê²Œ $S$ë¥¼ êµ¬í•˜ë©´, $(q_i, p_j)$ ì—ì„œ $i=j$ì¸ ê²½ìš°ì—ë§Œ `positive passage`ê°€ ë˜ê³ , ë‚˜ë¨¸ì§€ì˜ ê²½ìš°ì—ëŠ” `gold` ê¸°ë°˜ì˜ `negative passage`ê°€ ë˜ì–´ B-1ê°œì˜ `negative passage`ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. ë˜í•œ ì´ ë°©ì‹ì€ `computational efficency`ë¥¼ ë³´ì¥í•˜ê²Œ ëœë‹¤.

![image](https://github.com/user-attachments/assets/320a2166-33b0-44f4-b436-7a782ac06e12)

## 4. Experimental Setup ğŸ”¬

### 4.1 Wikipedia Data Pre-processing
ë”°ë¡œ ì¶”ê°€ì ì¸ ì„¤ëª…ì´ ë¶ˆí•„ìš”í•˜ì—¬ ì›ë¬¸ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™”ë‹¤. 
> Following (Lee et al., 2019), we use the English Wikipedia dump from Dec. 20, 2018 as the source documents for answering questions. We first apply the pre-processing code released in DrQA (Chen et al., 2017) to extract the clean, text-portion of articles from the Wikipedia dump. This step removes semi-structured data, such as tables, infoboxes, lists, as well as the disambiguation pages. We then split each article into multiple, disjoint text blocks of 100 words as passages, serving as our basic retrieval units, following (Wang et al., 2019), which results in 21,015,324 passages in the end.5 Each passage is also prepended with the title of the Wikipedia article where the passage is from, along with an [SEP] token.

### 4.1 Question Answering Datasets
ì´ ë…¼ë¬¸ì—ì„œëŠ” 5ê°€ì§€ QA datasetì„ ì´ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í–ˆë‹¤. ê·¸ ëª©ë¡ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. 

1. **Natural Questions (NQ)** : end-to-end QAì˜ ëª©ì ì— ë§ê²Œ design ë˜ì—ˆìœ¼ë©°, real Google search quesries ê³¼ Wikipediaì˜ answerì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆë‹¤. 
2. **TriviaQA** : Web ìƒì— ì¡´ì¬í•˜ëŠ” trivia questionsë¥¼ ì´ìš©í•˜ì—¬ ë§Œë“¤ì–´ì¡Œë‹¤. 
3. **WebQuestions (WQ)** : Google Suggest APIë¥¼ ì´ìš©í•˜ì—¬ answerë“¤ì€ Freebase ë‚´ë¶€ì— ìˆë„ë¡ í•˜ëŠ” dataset
4. **CuratedTREC (TREC)** : TREC QA trackê³¼ ì—¬ëŸ¬ Web soruceë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” open-domain QA dataset
5. **SQuAD v1.1** : Reading comprehensionì„ í†µí•´ ì–»ì€ ìœ ëª…í•œ dataset

![image](https://github.com/user-attachments/assets/44c3a124-9e34-4fa4-87cd-ef1d9b710777)

### 4.2 Selection of positive passages
`TREC`, `WebQuestions`, ê·¸ë¦¬ê³  `TriviaQA`ëŠ” ì ì€ ìˆ˜ì˜ `question-answer` ìŒì´ ì£¼ì–´ì¡Œê¸° ë•Œë¬¸ì—, `BM25`ë¥¼ ì´ìš©í•˜ì—¬ ê°€ì¥ answer ì´ ìˆì„ í™•ë¥ ì´ ë†’ì€ contextë¥¼ ì°¾ëŠ”ë‹¤. ë§Œì•½ ìƒìœ„ 100ê°œì˜ passageë“¤ ëª¨ë‘ ì •ë‹µì„ í¬í•¨í•˜ê³  ìˆì§€ ì•Šë‹¤ë©´, í•´ë‹¹ questionì€ ë¬´ì‹œë  ê²ƒì´ë‹¤. 

`SQuAD`ì™€ `Natural Questions`ë“¤ì— ëŒ€í•´ì„œëŠ”, ê¸°ì¡´ì˜ passageê°€ ë‚˜ë‰˜ì–´ì ¸ ìˆê³ , `candidate passage` ì™€ `pool` ë‚´ë¶€ê°€ ì„œë¡œ ë‹¤ë¥´ê²Œ processing ë˜ì—ˆê¸° ë•Œë¬¸ì—, ê° `gold passage`ë¥¼ ê·¸ì— ìƒì‘í•˜ëŠ” passageì™€ êµì²´ì‹œí‚¨ë‹¤. ë§Œì•½ ì´ ì‘ì—…ì— ì‹¤íŒ¨í•œë‹¤ë©´, ê·¸ ì§ˆë¬¸ì„ ì‚­ì œí•œë‹¤. 

## 5. Experiments: Passage Retrieval ğŸ§ª
ì´ ì„¹ì…˜ì—ì„œëŠ”, `retrieval performance` ë¥¼ ë‹¤ë£¬ë‹¤. ê¸°ì¡´ì˜ retrieval methodì— ëŒ€í•´ì„œ ì–´ë–¤ íš¨ê³¼ë¥¼ ê°€ì§€ëŠ”ì§€ì— ëŒ€í•´ì„œ ì‚´í´ë³¸ë‹¤. 

ë³¸ ë…¼ë¬¸ì˜ main experiementì—ì„œ ì‚¬ìš©ëœ DPR ëª¨ë¸ì€ batch sizeê°€ 128ì´ë©°, BM25 ê¸°ì¤€ì˜ `negative passage`ë¥¼ í¬í•¨í•œ `in-batch negative` ë¥¼ ì‚¬ìš©í•œë‹¤. ê·¸ë¦¬ê³  `question-passage` ìŒë“¤ì„ í° ë°ì´í„°ì…‹ (`NQ`, `TriviaQA`, `SQuAD`) ì— ëŒ€í•´ì„œëŠ” 40 epoch ë§Œí¼, ì‘ì€ ë°ì´í„°ì…‹ (`TREC`, `WQ`)ì— ëŒ€í•´ì„œëŠ” 100 epoch í•™ìŠµì‹œí‚¨ë‹¤. ë˜í•œ lr ì€ $10^{-5}$ ë¡œ ì„¤ì •í•˜ê³ , optimizerì€ `Adam` ì„ ì‚¬ìš©í–ˆë‹¤. (dropout : 0.1)

ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ ì˜ í•™ìŠµë˜ëŠ” retrieverì„ ìœ ì—°í•˜ê²Œ ë‹¤ë£¨ëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ, ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ ì „ë°˜ì ìœ¼ë¡œ ì¢‹ì€ í•™ìŠµë¥ ì„ ê°€ì§€ëŠ” retriever ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒ ë˜í•œ ì¢‹ì€ ì ‘ê·¼ì´ ë”œ ê²ƒì´ë‹¤. 

ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” `multi-dataset encoder`ì„ í•™ìŠµí•˜ê¸° ìœ„í•˜ì—¬ `SQuAD` ë°ì´í„°ì…‹ì„ ì œì™¸í•œ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì„ ë³‘í•©í•˜ì˜€ë‹¤. ë˜í•œ `BM25`, `BM25 + DPR`, `traditional retriever` ì„ ëª¨ë‘ ì‹¤í—˜í•´ë³´ì•˜ë‹¤. ì´ ê³¼ì •ì—ì„œ `BM25` ì™€ `DPR`ì˜ ê²°ê³¼ë¥¼ ê²°í•©í•˜ê¸° ìœ„í•´ì„œ $BM25(q, p) + \lambda * sim(q,p)$ ì™€ ê°™ì´ ì„ í˜•ì  ê²°í•©ìœ¼ë¡œ ëª¨ë¸ì„ êµ¬í˜„í•˜ì˜€ë‹¤.  ($\lambda = 1.1$ì¼ ë•Œê°€ ê°€ì¥ ì„±ëŠ¥ì´ ë†’ì•˜ë‹¤.)


### 5.1 Main Results

![image](https://github.com/user-attachments/assets/9e196fa6-314e-4487-9f98-7a5c30edeb4d)

ë³¸ ë…¼ë¬¸ì—ì„œ 5ê°€ì§€ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ `top-k passage`ë¥¼ ë½‘ì•„ë‚´ëŠ” `passage retrieval`ì„ ì§„í–‰í–ˆë‹¤. `SQuAD` datasetë¥¼ ì œì™¸í•˜ê³ , `DPR`ì€ `BM25` ë³´ë‹¤ ëª¨ë“  ë°ì´í„°ì…‹ì— ëŒ€í•´ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ê³ , kê°€ ì‘ì„ ë•Œ íŠ¹íˆ ë‹¤ë¥¸ ë°ì´í„°ì…‹ë“¤ ê°„ì˜ ì •í™•ë„ì˜ gapì´ ì»¤ì¡Œë‹¤. 

`multiple dataset`ì„ ì´ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•œ ê²°ê³¼, 5ê°€ì§€ ë°ì´í„°ì…‹ ì¤‘ ê°€ì¥ ì‘ì€ í¬ê¸°ë¥¼ ê°€ì§„ `TREC` dataset ì´ ë§¤ìš° í° ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ë‹¤. ë°˜ëŒ€ë¡œ, `Natural Questions` ì™€ `WQ` ëŠ” ì‘ì€ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì´ë©°, `TriviaQA`ì˜ ê²½ìš° ì˜¤íˆë ¤ ì¡°ê¸ˆ ë‚®ì•„ì§€ê¸°ë„ í–ˆë‹¤. ì´ ê²°ê³¼ëŠ” ì¶”í›„ `DPR` ê³¼ `BM25`ì˜ ê²°í•©ì— ì˜í•´ ë”ìš± í–¥ìƒë  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. 

> <span style='color:orange;'>**So why the SQuAD performs better in BM25?**</span><br> 1. anntoators ë“¤ì´ passageë¥¼ ë³¸ í›„ì— ì§ˆë¬¸ì„ ì‘ì„±í–ˆê¸° ë•Œë¬¸ì—, passageì˜ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆì„ í™•ë¥ ì´ ë†’ë‹¤. <br> 2. dataë“¤ì´ Wikipeidia ì—ì„œ 500ê°œ ì •ë„ì˜ ë°ì´í„°ë¥¼ ì¶”ì¶œí–ˆê¸° ë•Œë¬¸ì—, bias ê°€ ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. 

### 5.2 Ablation Study on Model Training
#### Sample efficiency
![image](https://github.com/user-attachments/assets/527a3369-ff23-4d97-bc17-1cc3441e01c1)

ê° `training dataset`ì˜ í¬ê¸°ì— ë”°ë¼ì„œ ì •í™•ë„ê°€ ë‹¬ë¼ì§€ê²Œ ë˜ëŠ”ë°, ê·¸ë˜í”„ì—ì„œ ë³´ë‹¤ì‹œí”¼ datasetì˜ í¬ê¸°ê°€ 1k ê°œë§Œ ë˜ì–´ë„ `BM25`ì˜ ì„±ëŠ¥ì„ ëŠ¥ê°€í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ë˜í•œ, retrieve í•˜ëŠ” `top-k` ìˆ˜ê°€ ì¦ê°€í• ìˆ˜ë¡ ì„±ëŠ¥ ë˜í•œ ì¦ê°€í•˜ê²Œ ëœë‹¤. 

#### In-batch negative training
![image](https://github.com/user-attachments/assets/a29f1515-85fe-4367-9a8c-f27e27a4550a)

ìœ„ í‘œëŠ” `negative passage`ì˜ ì„ ì • ë°©ì‹, `Negative passage`ì˜ ê°œìˆ˜, `In-batch negative` ì‚¬ìš© ìœ ë¬´, retrieve í•˜ëŠ” passageì˜ ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥ì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. 

ëšœë ·í•˜ê²Œ, #N(`negative passage`ì˜ ìˆ˜) ê°€ ì»¤ì§ì— ë”°ë¼ì„œ ì„±ëŠ¥ì´ ì¢‹ì•„ì§ì´ ë‚˜íƒ€ë‚¬ìœ¼ë©°, `Gold` ë°©ì‹ ë‹¨ì¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤, `BM25` ê¸°ì¤€ `negative passage`ë¥¼ 1ê°œì”© ì„ì–´ì„œ ì‚¬ìš©í•´ ì£¼ëŠ” ê²ƒì´ ë†’ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒˆë‹¤. 

í•˜ì§€ë§Œ, `BM25` ê¸°ì¤€ `negative passage`ì˜ ìˆ˜ë¥¼ 1ê°œì—ì„œ 2ê°œë¡œ ëŠ˜ë¦° ê²°ê³¼, ì„±ëŠ¥ì˜ ì°¨ì´ê°€ ê±°ì˜ ë‚˜íƒ€ë‚˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ë³´ì•„, `BM25`ì˜ ìˆ˜ëŠ” ëª¨ë¸ì— í¬ê²Œ ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤. 

> ê°œì¸ì ìœ¼ë¡œ ì¡°ê¸ˆ í¥ë¯¸ë¡œì› ë˜ ê²ƒì€, `In-batch negative passage method`ë¥¼ í™œìš©í•œ ê²°ê³¼ê°€ ë‹¨ì§€ `computational efficency`ë¥¼ ë³´ì¥í•˜ëŠ” ê²ƒ ë¿ë§Œ ì•„ë‹ˆë¼, ì„±ëŠ¥ì ì¸ ì¸¡ë©´ì—ì„œë„ ì˜ì˜ê°€ ìˆì—ˆë‹¤. 

#### Impact of gold passages
![image](https://github.com/user-attachments/assets/bfc4524d-0aab-4ce6-858a-a3959d9f6be2)

`Gold passage`ì˜ í•„ìš”ì„±ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í—˜ì„ í†µí•´ ì•Œì•„ëƒˆë‹¤. `Dist. Sup`ì€ `BM25`ì— ë”°ë¥¸ `negative passage`ë¥¼ ì˜ë¯¸í•˜ëŠ”ë°, ì´ì— ë¹„í•´ 1% ì •ë„ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì´ëŠ” ê²ƒìœ¼ë¡œ ë³´ì•„, `Gold passage`ê°€ ë”ìš± ì¢‹ì€ ë°©ë²•ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 

#### Similarity and loss
![image](https://github.com/user-attachments/assets/19a2d316-fc92-41af-ac88-fda7cb8c4672)

ë¨¼ì €, `similarity function` ì— ëŒ€í•´ì„œ ë‘ê°€ì§€ ë°©ì‹ì¸ `Dot Product`, `L2 distance` ë¥¼ ë¹„êµí•˜ê²Œ ëœë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ `Loss function` ì— ëŒ€í•´ì„œ 2ê°€ì§€ ë°©ì‹ì„ ë¹„êµí•˜ëŠ”ë°, `NLL (Negative log likelihood)`ì™€ `triplet loss` ë¥¼ ë¹„êµí•œë‹¤. 

ê²°ê³¼ì ìœ¼ë¡œ `Dot Product` ì™€ `NLL` ì„ ì‚¬ìš©í–ˆì„ ê²½ìš° retrieval ì„±ëŠ¥ì´ ì¢‹ì•˜ê¸° ë•Œë¬¸ì—, ì´ ë‘ ê°€ì§€ ë°©ì‹ì„ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ êµ¬ì„±í–ˆë‹¤.

#### Cross-datset generalization
ì´ì™¸ì—ë„, ì¶”ê°€ì ì¸ `fine-tuning` ì´ í•„ìš”í•˜ì§€ ì•Šë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ”, íŠ¹ì • dataset ì„ ì´ìš©í•´ í•™ìŠµí•œ ëª¨ë¸ì„ ë‹¤ë¥¸ datasetì— ì ìš©í•´ë´„ìœ¼ë¡œì¨ ì´ë¥¼ ì¦ëª…í•œë‹¤. `NQ` datasetì— ëŒ€í•´ì„œë§Œ `DPR`ë¥¼ í•™ìŠµì‹œí‚¨ í›„, `WQ`, `TREC`ì— ì‹¤í—˜ì„ í•´ë³¸ ê²°ê³¼, ìœ ì˜ë¯¸í•œ ê²°ê³¼ë¥¼ ë„ë©°, ìƒë‹¹íˆ ë†’ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤.


### 5.3 Qualitative Analysis
`BM25` ë³´ë‹¤ `DPR`ì´ ë” ë†’ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ê²ƒì—ë„ ë¶ˆêµ¬í•˜ê³ , `BM25`ê³¼ ê°™ì€ `Term-matching` ë°©ë²•ì€ íŠ¹ì • êµ¬ë‚˜ ì„ íƒì  í‚¤ì›Œë“œì— ëŒ€í•´ `sensitive` í•˜ë‹¤. ë°˜ë©´ `DPR`ì€ `semantic relationship`ì„ ë”ìš± ì˜ í‘œí˜„í•˜ê²Œ ëœë‹¤. 

### 5.4 Run-time Efficiency
> The main reason that we require a retrieval component for open-domain QA is to reduce the number of candidate passages that the reader needs to consider, which is crucial for answering userâ€™s questions in real-time. We profiled the passage retrieval speed on a server with Intel Xeon CPU E5-2698 v4 @ 2.20GHz and 512GB memory. With the help of FAISS in-memory index for real-valued vectors10, DPR can be made incredibly efficient, processing 995.0 questions per second, returning top 100 passages per question. In contrast, BM25/Lucene (implemented in Java, using file index) processes 23.7 questions per second per CPU thread.

> On the other hand, the time required for building an index for dense vectors is much longer. Computing dense embeddings on 21-million passages is resource intensive, but can be easily parallelized, taking roughly 8.8 hours on 8 GPUs. However, building the FAISS index on 21-million vectors on a single server takes 8.5 hours. In comparison, building an inverted index using Lucene is much cheaper and takes only about 30 minutes in total.

## 6. Experiments: Question Answering ğŸ§
### 6.1 End-to-end QA System
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì„œë¡œ ë‹¤ë¥¸ retriever systemì— ëŒ€í•´ì„œ ìœ ì—°í•˜ê²Œ ì ìš©í•  ìˆ˜ ìˆëŠ” `end-to-end QA system`ì„ êµ¬í˜„í–ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ `neural reader` ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. 

ë¨¼ì €, retrieverì´ `top-k retrieved passage`ë¥¼ ì œê³µí•˜ë©´, reader modelì€ passageë“¤ì— ëŒ€í•œ `selection score` ì„ ê° passageì— ë¶€ì—¬í•˜ê²Œ ëœë‹¤. ê·¸ë¦¬ê³  ê° passageë“¤ì— ëŒ€í•´ì„œ `answer span`ì„ ì¶”ì¶œí•˜ê³ , ê° span ë§ˆë‹¤ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ê²Œ ëœë‹¤. ê²°ë¡ ì ìœ¼ë¡œ ê°€ì¥ ë†’ì€ `passage score`ì—ì„œì˜ `answer span` ì´ ì •ë‹µìœ¼ë¡œ ì¶”ì¶œëœë‹¤. 

ì´ ê³¼ì •ì—ì„œ `passage selection model`ì€ `reranker` ì´ë¼ëŠ” ìƒˆë¡œìš´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ”ë°, question ê³¼ passage ê°„ì˜ `cross attention`ì„ ì´ìš©í•´ì„œ passage ê°„ì˜ `similarity` ë¥¼ ê³„ì‚°í•œë‹¤. ì´ ì—°ì‚°ì€ `decomposable` í•˜ê¸° ë•Œë¬¸ì— ë§ì€ passageì— ëŒ€í•´ì„œ ì ìš©í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ, `dual-encoder`ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ê¸° ë•Œë¬¸ì—, ì‘ì€ `top-k` ì— ëŒ€í•´ì„œ ì´ ê³¼ì •ì„ ìˆ˜í–‰í•œë‹¤. 

![image](https://github.com/user-attachments/assets/29e3eb4d-6d7f-4591-a23b-1abfa73345da)

ì‹ì€ ìœ„ì™€ ê°™ì€ë°, $\hat{P}$ ë¼ëŠ” ëª¨ë“  `Top-k passage` ì— ëŒ€í•´ì„œ `cross-attention` ì„ ìˆ˜í–‰í•˜ê³ , í•´ë‹¹ ê°’ì„ `softmax` ë¡œ í•™ìŠµì‹œì¼œ ê°€ì¥ ì—°ê´€ë„ê°€ ë†’ì€ passageë¥¼ ì°¾ëŠ”ë‹¤. ê·¸ í›„ì—, í•´ë‹¹ passageì—ì„œ $w$ ë¼ëŠ” `learnable vector`ì„ ì´ìš©í•´ì„œ `start token`, `end token` ì„ ê³±í•œ ê°’ì„ `answer span score` ë¡œ ì ìš©í•˜ì—¬ `answer span` ì„ ì°¾ê²Œ ëœë‹¤. 

`reader`ì˜ í•™ìŠµ ê³¼ì •ì€, `positive-passage` ì— ëŒ€í•œ `selection score`ì˜ `log-likelihood` ë¥¼ í†µí•´ í•™ìŠµë˜ë©°, `answer span`ì€ `positive passage`ì—ì„œì˜ ëª¨ë“  `answer span`ì˜ `marginal log-likelihood`ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµëœë‹¤. í•˜ë‚˜ì˜ passage ë‚´ë¶€ì—ì„œ ì •ë‹µì´ ì—¬ëŸ¬ ë²ˆ ë‚˜íƒ€ë‚  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ëª¨ë“  `answer span` ì— ëŒ€í•´ì„œ í•™ìŠµí•œë‹¤. 

### 6.2 Results
![image](https://github.com/user-attachments/assets/081f4d5b-c33a-483c-9eae-300e5900d6a5)

ê° Model, datasetì˜ í†µí•©ì„ ê¸°ì¤€ìœ¼ë¡œ ìœ„ì™€ ê°™ì´ ì •í™•ë„ë¥¼ ì¸¡ì •í–ˆë‹¤. ì „ì²´ì ìœ¼ë¡œ retrieverì˜ ì •í™•ë„ê°€ ë†’ì„ìˆ˜ë¡ `end-to-end` ì •í™•ë„ê°€ ë†’ì•„ì§„ë‹¤. ë‹¤ë¥¸ ëª¨ë¸ë“¤ (`ORQA`, `REALM`, etc)ì€ ëª¨ë¸ì„ ìœ„í•œ `pre-training`ì„ ìˆ˜í–‰í–ˆê³ , ë†’ì€ ê³„ì‚°ë³µì¡ë„ë¥¼ ì§€ë‹ˆê³  ìˆì§€ë§Œ, ë³¸ ë…¼ë¬¸ì˜ `DPR` ëª¨ë¸ì€ ì¶”ê°€ì ì¸ `pre-training` ì—†ì´ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•˜ì—¬ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

ì¶”ê°€ì ìœ¼ë¡œ, `Retrieval model` ê³¼ `reader model`ì„ `joint` í•˜ì—¬ ê°™ì€ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œì¨ ë™ì‹œì— í›ˆë ¨ì‹œí‚¤ëŠ” ì‹¤í—˜ ë˜í•œ í•´ë³´ì•˜ìœ¼ë‚˜, 39.8EMì„ ë‹¬ì„±í•˜ë©° ë…ë¦½ì ì¸ `retrieval`, `reader` modelì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë”ìš± ë†’ì€ ì„±ëŠ¥ì„ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤. 

## 8. Conclusion ğŸ¬
ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ `dense retrieval method`ê°€ ê¸°ì¡´ì˜ `traiditional sparse retrieveal componet` ë¥¼ ëŠ¥ê°€í•˜ê³ , ì ì¬ì ìœ¼ë¡œ ëŒ€ì²´í•˜ì˜€ë‹¤. ê°„ë‹¨í•˜ê²Œ `dual-encoder`ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ë†€ë¼ìš´ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ì´ ì†ì— ëª‡ëª‡ ì¤‘ìš” ìš”ì†Œë“¤ì´ ì¡´ì¬í•˜ê¸°ë„ í–ˆë‹¤. ë” ë‚˜ì•„ê°€ì„œ ì´ ë…¼ë¬¸ì—ì„œì˜ ì‹œí—˜ì  ë¶„ì„ì€, ë”ìš± ë³µì¡í•œ ëª¨ë¸ë“¤ì´ í•­ìƒ ì¶”ê°€ì ì¸ valueë¥¼ ì œê³µí•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë¼ëŠ” ê²ƒ ë˜í•œ ì•Œê²Œ ë˜ì—ˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ, ê²°êµ­ ë³¸ ë…¼ë¬¸ì˜ ë°©ì‹ìœ¼ë¡œ SOTAë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤. 