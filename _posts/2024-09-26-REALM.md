---
title: "[Paper Review] REALM: Retrieval-Augmented Language Model Pre-Training"
description: "Paper Review for REALM"
writer: Sangyun Won
categories: [AI, Paper Review]
tags: [AI]
image:
  path: https://github.com/user-attachments/assets/e0f3409d-c116-47f5-bab3-0fc367d9e2cf
  alt: Paper Review for REALM

math: true
toc: true
toc_sticky: true

date: 2024-09-26
last_modified_at: 2024-09-26
---

<style>
  figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}
.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}
</style>

<figure>
  <a href="https://arxiv.org/pdf/2002.08909" class="bookmark source">
    <div class="bookmark-info">
      <div class="bookmark-text">
        <div class="bookmark-title">REALM: Retrieval-Augmented Language Model Pre-Training</div>
        <div class="bookmark-description">arxiv pdf link for REALM</div>
      </div>
    </div>
  </a>
</figure>

## 0. Abstract
지금까지 여러 LM pre-training 은 엄청나게 많은 양의 지식을 학습하기 위해서 진행되어 왔으며, 이는 NLP task 중에서도 QA task 에 매우 중대한 영향을 주었다. 하지만, 이 지식들은 암시적으로 모델의 파라미터에 저장되어 있으며, 더 많은 지식을 학습하기 위해서는 더 큰 network 이 필요하다. 지식을 더욱 해석 가능하고, 일반적으로 저장하기 위해서는, latent knowledge retriever, 즉 다른 large corpus 내부의 document 에서 retrieve 해주는 모델이 필요하다. 

처음으로 본 논문에서는 masked language 모델링 방식으로 knowledge retriever 을 unsupervised 방식으로 학습시키는 것을 보일 것이고, 이를 역전파를 통해 학습하는 과정까지 보일 것이다. 본 논문에서는 Retrieval-Augmented Language Model pre-training 을 OPQA 에 적용시킬 것이고, 명시적 지식과 암시적 지식에 대해서 다른 SOTA 몯델들과 비교하여 다른 모델들의 성능을 능가함을 보일 것이다. 

## 1. Introduction
최근 BERT, RoBERTa, T5와 같은 다양한 LM 의 pre-training은 엄청난 양의 world knowledge 를 저장하고 있으며, 이는 그 모델들이 학습한 엄청난 양의 text 데이터에 기반한다. 

예를 들어
> "The ____ is the currency of the United Kingdom". (answer : pound)

이라는 문장에서 손쉽게 마스킹된 단어를 예측해낼 수 있다. 

이런 LM들의 경우, world knowledgge 를 파라미터를 통해 암시적으로 저장하며, 이는 그 지식이 어떻게, 어디에 저장되어 있는지를 파악하기가 어렵다. 더 나아가, store space 는 한정적이기 때문에, 더 많은 지식을 위해서는 더 큰 네트워크가 필수적이라고 할 수 있다. 

<p align="center"><img src="https://github.com/user-attachments/assets/e0f3409d-c116-47f5-bab3-0fc367d9e2cf" height="400px" width=500px"></p>

보다 더 많은 지식을 저장하고, 해석 가능한 방식을 위해서 본 논문에서는 REALM (Retrieval-Augmented Language Model) 을 소개한다. 이 방식은 learned textual knowledge retriever 을 사용하여 모델의 pre-traing 방식을 증진시킨다. 다른 모델들과는 반대로 이 모델은 inference 시에 어떤 정보를 retrieve 하고 사용할 것인지를 요청하여 지식의 명시적 접근을 적용한다. 각 prediction 전에, retriever 을 사용하여 매우 큰 corpus 내부의 documnet 를 검색하게 되고, 이 document 를 활용하여 질문에 대한 답을 생성해낸다. 이 모델을 end-to-end 방식으로 학습하는 것은 retriever 에 대한 backpropagation 을 요구하며, 이는 곧 전체 corpus 에 대한 접근이 필요하다. 

이 REALM 방식에 대한 key intuition 은 retriever 을 학습시킬 때, unsupervised-text 기반의 performance-based signal 을 사용하는 것이다. 이 방식은 LM의 perplexity 를 향상시키는 retrieval 에 대해서 reward 를 부여하게 된다. 

예를 들어, 
> "the ____ at the top of the pyramid" 

라는 문장이 존재하고, 마스킹된 토큰을 predict 하고자 할 때, retriever은 아래와 같은 document 를 retrieve 할 때 rewarded 되게 된다. 

> "The pyramidion on top allows for less material higher up the pyramid"

본 논문에서는 latent variable LM에 대한 retrieve-then-predict 접근의 모델링을 통해서 marginal likelihood 를 최적화시키는 방향을 학습하게 된다. 