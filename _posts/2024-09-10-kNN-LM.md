---
title: "[Paper Review] Generalization Through Memorization: Nearest Neighbor Language Models"
description: "Paper Review for kNN-LM model"
writer: Sangyun Won
categories: [AI, Paper Review]
tags: [AI]
image:
  path: https://github.com/user-attachments/assets/f44a25a1-dc34-46d6-b024-8a1a881f5953
  alt: Paper Review for kNN-LM model

math: true
toc: true
toc_sticky: true

date: 2024-09-10
last_modified_at: 2024-09-10
---

## 0. Abstract 🎬

`kNN-LM` 은 `pre-train` 된 LM을 선형적으로 kNN 알고리즘과 결합하여 확장한 모델이다. `pre-trained LM`을 이용해서 input 데이터가 `latent space` 로 임베딩 되게 되는데, 이 `latent space` 상의 벡터 간의 거리를 통해서 가장 가까운 k 개의 후보를 정하게 된다. 이는 임의의 데이터셋 (including training data) 을 통해서 가능하다.

이 방식을 `Wikitext-103LM` 에 적용함으로써, 이 논문에서 소개하는 모델은 SOTA를 달성했으며, 추가적인 training 없이도 15.79의 `perplexity`로 2.9 point나 줄이는 효과를 보였다. 또한, 이 접근법은 더 큰 훈련 데이터셋, 그리고 다른 `domain` 으로의 적용 역시 효과적으로 수행할 수 있었다.

질적으로는, 이 모델은 생소한 표현들에 대해서 더욱 효과적인 모습을 보였고, 특히 `factual knowledege` 에 대해서 효과적이였다. 동시에 이 연구는 LM 의 근본적인 task 인 `next token prediction` 보다 `sequences 간의 similarity` 를 학습하는 것이 더 효과적인 접근 방식임을 의미하기도 한다.

## 1. Introduction ☕️

Language Model 은 일반적으로 아래의 2가지 task를 목표로 한다.

**1. 즉 문장의 prefix를 n차원 벡터로 나타낸다. (정확히는 고정 크기의 representation)**

**2. 이렇게 만들어진 latent space에서의 값을 이용해서 다음 단어를 예측한다.**

본 논문에서는 첫번째 task가 두번째 task 보다 쉬운 task라는 가정 하에 접근했다. 예를 들어, "_Dickens is the author of_" 라는 문장과 "_Dickens wrote_" 라는 문장을 보았을 때, 그 후에 올 단어를 예측하지 못하더라도 두 문장이 같은 뜻을 내포하고 있음은 누구나 알 수 있다. 실험적으로도, `prefix embedding`에 대해 kNN을 적용시킨 결과, 성능이 향상됨을 통해 LM이 첫번째 task에 더 효과적이라는 강력한 증거를 제시한다.

`3-billion`개의 `token`을 모델의 학습 데이터로 사용하는 것보다, `100-million`개의 `token`을 이용해 학습하고 3-billion개의 `token`을 가지는 `dataset(documents)`을 이 모델에 적용하는 것이 더 높은 성능을 보였다. 이는 곧 Large dataset을 사용하는 LM에 대한 새로운 방향성을 제시한다. 비슷하게, 단순히 `datastore` 에 다른 domain의 데이터를 삽입하는 것만으로도 `multiple domain`에서도 효과적인 성능을 보였다.

![image](https://github.com/user-attachments/assets/f44a25a1-dc34-46d6-b024-8a1a881f5953)

마지막으로, 이 모델은 명시적 기억에 대한 접근 (datastore이라는 명시적인 데이터) 을 통해 `long-tail patterns` (예를 들어 `Factual Knowledge`) 에 대해서 더욱 효과적인 것을 발견했다.

## 2. Nearest Neighbor Language Modeling 🧐

LM은 기본적으로 `sequence`에 대한 확률을 할당한다. 다시 말해 $c_t = (w_1, \cdots , w_{t-1})$ 라는 `context (sequence)`가 주어져 있을 때, LM (autoregressive 한)은
$p(w_t|c_t)$를 계산해낸다.

*k*NN-LM은 `pre-trained LM`을 이용하여 `nearest-neighbors`를 검색(`retrieval`) 하여 `augument` 하는 과정을 내포하며, 이 datastore에는 `key-value` 형태의 `context-target` 쌍들이 저장되어 있다. (see Figure 1)

### Datastore

$f()$를 context $c$를 `fixed-length vector`로 `mapping`해주는 함수라고 가정해보자. 만약에 i번째 training data인 $(c_i, w_i) \in \mathcal{D}$가 주어졌을 때, 우리는 `datastore`을 다음과 같은 `key-value` 집합으로 정의할 수 있다.

$$
(\mathcal{K}, \mathcal{V}) = \{(f(c_i), w_i)|(c_i, w_i)\} \in \mathcal{D}
$$

### Inference

이 모델은 `input context` $x$가 주어졌을 때, $f(x)$를 통해 $p_{LM}(y|x)$에 대한 확률 분포를 계산하게 된다. 동시에 datastore 에 $f(x)$를 이용해 query를 보내게 되는데, `distance function`인 $d()$를 통해 `k-nearest neighbors`에 해당하는 집합 $\mathcal{N}$을 생성한다.
(본 논문에서는 `distance function`을 $L^2$ distance로 정의했다.)

그 후에는 `softmax`에 `negative distance`를 넣음으로써, 아래의 확률을 계산해낼 수 있다. (거리가 가까울수록 높은 확률의 정확도를 보인다.)

$$
p*{kNN}(y|x) \propto \sum_{(k*i, v_i) \in \mathcal{N}}{\mathbb{1}_{y=v_i} \, exp(-d(k_i, f(x)))}
$$

이를 선형적으로 기존 LM에 적용하게 되면, $\lambda$ 변수를 이용해서 다음과 같이 최종 `probability`를 정의할 수 있다.

$$p(y|x) = \lambda * p_{kNN}(y|x) + (1 - \lambda) * p_{LM}(y|x)$$

### Implementation

한 가지 문제점은, `Datastore` 이 `billion` 개의 데이터를 내포하고 있어 `computationally intensive` 하다는 것이다. 이를 극복하기 위해 `FAISS`라는 `open source library` 를 사용하여 고차원 상에서의 kNN을 효율적으로 계산하게 된다. 추가적으로, $L^2$ 이외에 `inner product distance` 라는 `distance function` 또한 존재하는데, 이 모델에서는 $L^2$ 방식이 더 높은 성능을 보였다.

### Related Cache Models

이전의 비슷한 접근 방식에서는, recent past에 대한 `caching` 을 통해서 최근의 데이터에 대해 더욱 효과적으로 계산하는 방식 또한 존재했다. 하지만 최근의 정보를 copy할 수 있는 `self-attention` 기법을 가진 `Transformer` 모델이 등장하고 나서, 이 방식은 인기를 잃게 되며, 얻을 수 있는 이익 또한 줄어들었다. 본 논문에서는 `training data`에 대한 명시적 기억을 위해 오로지 `training data`에 대해서만 `caching`하는 방식을 택하여 비슷한 cases에 대한 효율을 증대시켰다.

## 3. Experimental Setup 🥽

### Data

단순히 dataset에 대한 설명이기에 별도의 설명은 생략한다.

![image](https://github.com/user-attachments/assets/61a7608f-8383-4799-b01c-fdd0bd523ff9)

### Model Architecture

*k*NN-LM 모델은 `fixed-size context representations`을 생성하는 모델이라면 모두 호환이 가능하다. 이 모델은 현재 (당시) SOTA를 기록했던 `Decoder-only Transformer`을 사용한다. *k*NN-LM 모델은 모델의 기본 모델인 LM에 대한 훈련을 시행하지 않기 때문에, 기존 아키텍쳐와 최적화 방식을 그대로 사용했다.

`16 layer`, `each with 16 self-attention heads`, `1024 dimensional hidden states`, `4096 dimensional feedforward layers`, `247M parameters` 등을 사용하며 추가 정보는 아래와 같다.

![image](https://github.com/user-attachments/assets/0389fbaa-e033-4f79-9977-b2956de075fd)

### Evaluation

이 LM들은 `negative log-likelihood` 를 `loss function`으로 사용했으며, 모델의 평가 기준으로써 `perplexity`를 사용했다. (_살짝 첨언하자면, 최근 논문인 Mamba의 reject 원인으로 `perplexity`가 평가 기준이 되는 것은 정당하지 않다는 글을 본 것 같은데, 이때는 기준이 조금 달랐나보다._)

### *k*NN-LM

<head>
  <meta property="og:title" content="Your Blog Title" />
  <meta property="og:description" content="A description of your blog or link." />
  <meta property="og:image" content="https://www.google.com/url?sa=i&url=https%3A%2F%2Fko.m.wikipedia.org%2Fwiki%2F%25ED%258C%258C%25EC%259D%25BC%3ACc.logo.circle.svg&psig=AOvVaw1JoW2iBPCKajjYfedhSweQ&ust=1726653799993000&source=images&cd=vfe&opi=89978449&ved=0CBQQjRxqFwoTCKCBn-XcyYgDFQAAAAAdAAAAABAE" />
  <meta property="og:url" content="https://comibear.kr" />
  <meta name="twitter:card" content="summary_large_image" />
</head>
