---
title: "[Paper Review] Generalization Through Memorization: Nearest Neighbor Language Models"
description: "Paper Review for kNN-LM model"
writer: Sangyun Won
categories: [AI, Paper Review]
tags: [AI]
image:
  path: https://github.com/user-attachments/assets/f44a25a1-dc34-46d6-b024-8a1a881f5953
  alt: Paper Review for kNN-LM model

math: true
toc: true
toc_sticky: true

date: 2024-09-10
last_modified_at: 2024-09-10
---

<style>
  figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}
.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}
</style>

## 0. Abstract 🎬

`kNN-LM` 은 `pre-train` 된 LM을 선형적으로 kNN 알고리즘과 결합하여 확장한 모델이다. `pre-trained LM`을 이용해서 input 데이터가 `latent space` 로 임베딩 되게 되는데, 이 `latent space` 상의 벡터 간의 거리를 통해서 가장 가까운 k 개의 후보를 정하게 된다. 이는 임의의 데이터셋 (including training data) 을 통해서 가능하다.

이 방식을 `Wikitext-103LM` 에 적용함으로써, 이 논문에서 소개하는 모델은 SOTA를 달성했으며, 추가적인 training 없이도 15.79의 `perplexity`로 2.9 point나 줄이는 효과를 보였다. 또한, 이 접근법은 더 큰 훈련 데이터셋, 그리고 다른 `domain` 으로의 적용 역시 효과적으로 수행할 수 있었다.

질적으로는, 이 모델은 생소한 표현들에 대해서 더욱 효과적인 모습을 보였고, 특히 `factual knowledege` 에 대해서 효과적이였다. 동시에 이 연구는 LM 의 근본적인 task 인 `next token prediction` 보다 `sequences 간의 similarity` 를 학습하는 것이 더 효과적인 접근 방식임을 의미하기도 한다.

## 1. Introduction ☕️

Language Model 은 일반적으로 아래의 2가지 task를 목표로 한다.

**1. 즉 문장의 prefix를 n차원 벡터로 나타낸다. (정확히는 고정 크기의 representation)**

**2. 이렇게 만들어진 latent space에서의 값을 이용해서 다음 단어를 예측한다.**

본 논문에서는 첫번째 task가 두번째 task 보다 쉬운 task라는 가정 하에 접근했다. 예를 들어, "_Dickens is the author of_" 라는 문장과 "_Dickens wrote_" 라는 문장을 보았을 때, 그 후에 올 단어를 예측하지 못하더라도 두 문장이 같은 뜻을 내포하고 있음은 누구나 알 수 있다. 실험적으로도, `prefix embedding`에 대해 kNN을 적용시킨 결과, 성능이 향상됨을 통해 LM이 첫번째 task에 더 효과적이라는 강력한 증거를 제시한다.

`3-billion`개의 `token`을 모델의 학습 데이터로 사용하는 것보다, `100-million`개의 `token`을 이용해 학습하고 3-billion개의 `token`을 가지는 `dataset(documents)`을 이 모델에 적용하는 것이 더 높은 성능을 보였다. 이는 곧 Large dataset을 사용하는 LM에 대한 새로운 방향성을 제시한다. 비슷하게, 단순히 `datastore` 에 다른 domain의 데이터를 삽입하는 것만으로도 `multiple domain`에서도 효과적인 성능을 보였다.

![image](https://github.com/user-attachments/assets/f44a25a1-dc34-46d6-b024-8a1a881f5953)

마지막으로, 이 모델은 명시적 기억에 대한 접근 (datastore이라는 명시적인 데이터) 을 통해 `long-tail patterns` (예를 들어 `Factual Knowledge`) 에 대해서 더욱 효과적인 것을 발견했다.

## 2. Nearest Neighbor Language Modeling 🧐

LM은 기본적으로 `sequence`에 대한 확률을 할당한다. 다시 말해 $c_t = (w_1, \cdots , w_{t-1})$ 라는 `context (sequence)`가 주어져 있을 때, LM (autoregressive 한)은
$p(w_t|c_t)$를 계산해낸다.

*k*NN-LM은 `pre-trained LM`을 이용하여 `nearest-neighbors`를 검색(`retrieval`) 하여 `augument` 하는 과정을 내포하며, 이 datastore에는 `key-value` 형태의 `context-target` 쌍들이 저장되어 있다. (see Figure 1)

### Datastore

$f()$를 context $c$를 `fixed-length vector`로 `mapping`해주는 함수라고 가정해보자. 만약에 i번째 training data인 $(c_i, w_i) \in \mathcal{D}$가 주어졌을 때, 우리는 `datastore`을 다음과 같은 `key-value` 집합으로 정의할 수 있다.

$$
(\mathcal{K}, \mathcal{V}) = \{(f(c_i), w_i)|(c_i, w_i)\} \in \mathcal{D}
$$

### Inference

이 모델은 `input context` $x$가 주어졌을 때, $f(x)$를 통해 $p_{LM}(y|x)$에 대한 확률 분포를 계산하게 된다. 동시에 datastore 에 $f(x)$를 이용해 query를 보내게 되는데, `distance function`인 $d()$를 통해 `k-nearest neighbors`에 해당하는 집합 $\mathcal{N}$을 생성한다.
(본 논문에서는 `distance function`을 $L^2$ distance로 정의했다.)

그 후에는 `softmax`에 `negative distance`를 넣음으로써, 아래의 확률을 계산해낼 수 있다. (거리가 가까울수록 높은 확률의 정확도를 보인다.)

$$
p*{kNN}(y|x) \propto \sum_{(k*i, v_i) \in \mathcal{N}}{\mathbb{1}_{y=v_i} \, exp(-d(k_i, f(x)))}
$$

이를 선형적으로 기존 LM에 적용하게 되면, $\lambda$ 변수를 이용해서 다음과 같이 최종 `probability`를 정의할 수 있다.

$$p(y|x) = \lambda * p_{kNN}(y|x) + (1 - \lambda) * p_{LM}(y|x)$$

### Implementation

한 가지 문제점은, `Datastore` 이 `billion` 개의 데이터를 내포하고 있어 `computationally intensive` 하다는 것이다. 이를 극복하기 위해 `FAISS`라는 `open source library` 를 사용하여 고차원 상에서의 kNN을 효율적으로 계산하게 된다. 추가적으로, $L^2$ 이외에 `inner product distance` 라는 `distance function` 또한 존재하는데, 이 모델에서는 $L^2$ 방식이 더 높은 성능을 보였다.

### Related Cache Models

이전의 비슷한 접근 방식에서는, recent past에 대한 `caching` 을 통해서 최근의 데이터에 대해 더욱 효과적으로 계산하는 방식 또한 존재했다. 하지만 최근의 정보를 copy할 수 있는 `self-attention` 기법을 가진 `Transformer` 모델이 등장하고 나서, 이 방식은 인기를 잃게 되며, 얻을 수 있는 이익 또한 줄어들었다. 본 논문에서는 `training data`에 대한 명시적 기억을 위해 오로지 `training data`에 대해서만 `caching`하는 방식을 택하여 비슷한 cases에 대한 효율을 증대시켰다.

## 3. Experimental Setup 🥽

### Data

단순히 dataset에 대한 설명이기에 별도의 설명은 생략한다.

![image](https://github.com/user-attachments/assets/61a7608f-8383-4799-b01c-fdd0bd523ff9)

### Model Architecture

*k*NN-LM 모델은 `fixed-size context representations`을 생성하는 모델이라면 모두 호환이 가능하다. 이 모델은 현재 (당시) SOTA를 기록했던 `Decoder-only Transformer`을 사용한다. *k*NN-LM 모델은 모델의 기본 모델인 LM에 대한 훈련을 시행하지 않기 때문에, 기존 아키텍쳐와 최적화 방식을 그대로 사용했다.

`16 layer`, `each with 16 self-attention heads`, `1024 dimensional hidden states`, `4096 dimensional feedforward layers`, `247M parameters` 등을 사용하며 추가 정보는 아래와 같다.

![image](https://github.com/user-attachments/assets/0389fbaa-e033-4f79-9977-b2956de075fd)

### Evaluation

이 LM들은 `negative log-likelihood` 를 `loss function`으로 사용했으며, 모델의 평가 기준으로써 `perplexity`를 사용했다. (_살짝 첨언하자면, 최근 논문인 Mamba의 reject 원인으로 `perplexity`가 평가 기준이 되는 것은 정당하지 않다는 글을 본 것 같은데, 이때는 기준이 조금 달랐나보다._)

### *k*NN-LM

<figure id="8b48f93f-7612-4d74-8935-a028926a8762">
  <a href="https://facerain.github.io/nlp-tutorial-roadmap/" class="bookmark source">
    <div class="bookmark-info">
      <div class="bookmark-text">
        <div class="bookmark-title">[5분 NLP] NLP 공부 시작하기</div>
        <div class="bookmark-description">NLP 공부 어떻게 시작하면 좋을까요?</div>
      </div>
    </div>
    <img src="https://facerain.github.io/static/e14d9a9714a527a3420c0312b381ee9f/th.jpg" class="bookmark-image" alt="img"/>
  </a>
</figure>
