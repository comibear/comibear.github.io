---
title: "[Paper Review] Generalization Through Memorization: Nearest Neighbor Language Models"
description: "Paper Review for kNN-LM model"
writer: Sangyun Won
categories: [AI, Paper Review]
tags: [AI]
image:
  path: https://github.com/user-attachments/assets/f44a25a1-dc34-46d6-b024-8a1a881f5953
  alt: Paper Review for kNN-LM model

math: true
toc: true
toc_sticky: true

date: 2024-09-10
last_modified_at: 2024-09-17
---

<style>
  figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}
.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}
</style>

<figure>
  <a href="https://arxiv.org/pdf/1911.00172" class="bookmark source">
    <div class="bookmark-info">
      <div class="bookmark-text">
        <div class="bookmark-title">Generalization Through Memorization: Nearest Neighbor Language Models</div>
        <div class="bookmark-description">arxiv pdf link for kNN-LM</div>
      </div>
    </div>
  </a>
</figure>

## 0. Abstract ğŸ¬

`kNN-LM` ì€ `pre-train` ëœ LMì„ ì„ í˜•ì ìœ¼ë¡œ kNN ì•Œê³ ë¦¬ì¦˜ê³¼ ê²°í•©í•˜ì—¬ í™•ì¥í•œ ëª¨ë¸ì´ë‹¤. `pre-trained LM`ì„ ì´ìš©í•´ì„œ input ë°ì´í„°ê°€ `latent space` ë¡œ ì„ë² ë”© ë˜ê²Œ ë˜ëŠ”ë°, ì´ `latent space` ìƒì˜ ë²¡í„° ê°„ì˜ ê±°ë¦¬ë¥¼ í†µí•´ì„œ ê°€ì¥ ê°€ê¹Œìš´ k ê°œì˜ í›„ë³´ë¥¼ ì •í•˜ê²Œ ëœë‹¤. ì´ëŠ” ì„ì˜ì˜ ë°ì´í„°ì…‹ (including training data) ì„ í†µí•´ì„œ ê°€ëŠ¥í•˜ë‹¤.

ì´ ë°©ì‹ì„ `Wikitext-103LM` ì— ì ìš©í•¨ìœ¼ë¡œì¨, ì´ ë…¼ë¬¸ì—ì„œ ì†Œê°œí•˜ëŠ” ëª¨ë¸ì€ SOTAë¥¼ ë‹¬ì„±í–ˆìœ¼ë©°, ì¶”ê°€ì ì¸ training ì—†ì´ë„ 15.79ì˜ `perplexity`ë¡œ 2.9 pointë‚˜ ì¤„ì´ëŠ” íš¨ê³¼ë¥¼ ë³´ì˜€ë‹¤. ë˜í•œ, ì´ ì ‘ê·¼ë²•ì€ ë” í° í›ˆë ¨ ë°ì´í„°ì…‹, ê·¸ë¦¬ê³  ë‹¤ë¥¸ `domain` ìœ¼ë¡œì˜ ì ìš© ì—­ì‹œ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆì—ˆë‹¤.

ì§ˆì ìœ¼ë¡œëŠ”, ì´ ëª¨ë¸ì€ ìƒì†Œí•œ í‘œí˜„ë“¤ì— ëŒ€í•´ì„œ ë”ìš± íš¨ê³¼ì ì¸ ëª¨ìŠµì„ ë³´ì˜€ê³ , íŠ¹íˆ `factual knowledege` ì— ëŒ€í•´ì„œ íš¨ê³¼ì ì´ì˜€ë‹¤. ë™ì‹œì— ì´ ì—°êµ¬ëŠ” LM ì˜ ê·¼ë³¸ì ì¸ task ì¸ `next token prediction` ë³´ë‹¤ `sequences ê°„ì˜ similarity` ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ë” íš¨ê³¼ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ì„ ì˜ë¯¸í•˜ê¸°ë„ í•œë‹¤.

## 1. Introduction â˜•ï¸

Language Model ì€ ì¼ë°˜ì ìœ¼ë¡œ ì•„ë˜ì˜ 2ê°€ì§€ taskë¥¼ ëª©í‘œë¡œ í•œë‹¤.

**1. ì¦‰ ë¬¸ì¥ì˜ prefixë¥¼ nì°¨ì› ë²¡í„°ë¡œ ë‚˜íƒ€ë‚¸ë‹¤. (ì •í™•íˆëŠ” ê³ ì • í¬ê¸°ì˜ representation)**

**2. ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ latent spaceì—ì„œì˜ ê°’ì„ ì´ìš©í•´ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•œë‹¤.**

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì²«ë²ˆì§¸ taskê°€ ë‘ë²ˆì§¸ task ë³´ë‹¤ ì‰¬ìš´ taskë¼ëŠ” ê°€ì • í•˜ì— ì ‘ê·¼í–ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, "_Dickens is the author of_" ë¼ëŠ” ë¬¸ì¥ê³¼ "_Dickens wrote_" ë¼ëŠ” ë¬¸ì¥ì„ ë³´ì•˜ì„ ë•Œ, ê·¸ í›„ì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ì§€ ëª»í•˜ë”ë¼ë„ ë‘ ë¬¸ì¥ì´ ê°™ì€ ëœ»ì„ ë‚´í¬í•˜ê³  ìˆìŒì€ ëˆ„êµ¬ë‚˜ ì•Œ ìˆ˜ ìˆë‹¤. ì‹¤í—˜ì ìœ¼ë¡œë„, `prefix embedding`ì— ëŒ€í•´ kNNì„ ì ìš©ì‹œí‚¨ ê²°ê³¼, ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ í†µí•´ LMì´ ì²«ë²ˆì§¸ taskì— ë” íš¨ê³¼ì ì´ë¼ëŠ” ê°•ë ¥í•œ ì¦ê±°ë¥¼ ì œì‹œí•œë‹¤.

`3-billion`ê°œì˜ `token`ì„ ëª¨ë¸ì˜ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤, `100-million`ê°œì˜ `token`ì„ ì´ìš©í•´ í•™ìŠµí•˜ê³  3-billionê°œì˜ `token`ì„ ê°€ì§€ëŠ” `dataset(documents)`ì„ ì´ ëª¨ë¸ì— ì ìš©í•˜ëŠ” ê²ƒì´ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. ì´ëŠ” ê³§ Large datasetì„ ì‚¬ìš©í•˜ëŠ” LMì— ëŒ€í•œ ìƒˆë¡œìš´ ë°©í–¥ì„±ì„ ì œì‹œí•œë‹¤. ë¹„ìŠ·í•˜ê²Œ, ë‹¨ìˆœíˆ `datastore` ì— ë‹¤ë¥¸ domainì˜ ë°ì´í„°ë¥¼ ì‚½ì…í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ `multiple domain`ì—ì„œë„ íš¨ê³¼ì ì¸ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

![image](https://github.com/user-attachments/assets/f44a25a1-dc34-46d6-b024-8a1a881f5953)

ë§ˆì§€ë§‰ìœ¼ë¡œ, ì´ ëª¨ë¸ì€ ëª…ì‹œì  ê¸°ì–µì— ëŒ€í•œ ì ‘ê·¼ (datastoreì´ë¼ëŠ” ëª…ì‹œì ì¸ ë°ì´í„°) ì„ í†µí•´ `long-tail patterns` (ì˜ˆë¥¼ ë“¤ì–´ `Factual Knowledge`) ì— ëŒ€í•´ì„œ ë”ìš± íš¨ê³¼ì ì¸ ê²ƒì„ ë°œê²¬í–ˆë‹¤.

## 2. Nearest Neighbor Language Modeling ğŸ§

LMì€ ê¸°ë³¸ì ìœ¼ë¡œ `sequence`ì— ëŒ€í•œ í™•ë¥ ì„ í• ë‹¹í•œë‹¤. ë‹¤ì‹œ ë§í•´ $c_t = (w_1, \cdots , w_{t-1})$ ë¼ëŠ” `context (sequence)`ê°€ ì£¼ì–´ì ¸ ìˆì„ ë•Œ, LM (autoregressive í•œ)ì€
$p(w_t|c_t)$ë¥¼ ê³„ì‚°í•´ë‚¸ë‹¤.

*k*NN-LMì€ `pre-trained LM`ì„ ì´ìš©í•˜ì—¬ `nearest-neighbors`ë¥¼ ê²€ìƒ‰(`retrieval`) í•˜ì—¬ `augument` í•˜ëŠ” ê³¼ì •ì„ ë‚´í¬í•˜ë©°, ì´ datastoreì—ëŠ” `key-value` í˜•íƒœì˜ `context-target` ìŒë“¤ì´ ì €ì¥ë˜ì–´ ìˆë‹¤. (see Figure 1)

### Datastore

$f()$ë¥¼ context $c$ë¥¼ `fixed-length vector`ë¡œ `mapping`í•´ì£¼ëŠ” í•¨ìˆ˜ë¼ê³  ê°€ì •í•´ë³´ì. ë§Œì•½ì— ië²ˆì§¸ training dataì¸ $(c_i, w_i) \in \mathcal{D}$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ìš°ë¦¬ëŠ” `datastore`ì„ ë‹¤ìŒê³¼ ê°™ì€ `key-value` ì§‘í•©ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

$$
(\mathcal{K}, \mathcal{V}) = \{(f(c_i), w_i)|(c_i, w_i)\} \in \mathcal{D}
$$

### Inference

ì´ ëª¨ë¸ì€ `input context` $x$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, $f(x)$ë¥¼ í†µí•´ $p_{LM}(y|x)$ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ê³„ì‚°í•˜ê²Œ ëœë‹¤. ë™ì‹œì— datastore ì— $f(x)$ë¥¼ ì´ìš©í•´ queryë¥¼ ë³´ë‚´ê²Œ ë˜ëŠ”ë°, `distance function`ì¸ $d()$ë¥¼ í†µí•´ `k-nearest neighbors`ì— í•´ë‹¹í•˜ëŠ” ì§‘í•© $\mathcal{N}$ì„ ìƒì„±í•œë‹¤.
(ë³¸ ë…¼ë¬¸ì—ì„œëŠ” `distance function`ì„ $L^2$ distanceë¡œ ì •ì˜í–ˆë‹¤.)

ê·¸ í›„ì—ëŠ” `softmax`ì— `negative distance`ë¥¼ ë„£ìŒìœ¼ë¡œì¨, ì•„ë˜ì˜ í™•ë¥ ì„ ê³„ì‚°í•´ë‚¼ ìˆ˜ ìˆë‹¤. (ê±°ë¦¬ê°€ ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ í™•ë¥ ì˜ ì •í™•ë„ë¥¼ ë³´ì¸ë‹¤.)

$$
p*{kNN}(y|x) \propto \sum_{(k*i, v_i) \in \mathcal{N}}{\mathbb{1}_{y=v_i} \, exp(-d(k_i, f(x)))}
$$

ì´ë¥¼ ì„ í˜•ì ìœ¼ë¡œ ê¸°ì¡´ LMì— ì ìš©í•˜ê²Œ ë˜ë©´, $\lambda$ ë³€ìˆ˜ë¥¼ ì´ìš©í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ìµœì¢… `probability`ë¥¼ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

$$p(y|x) = \lambda * p_{kNN}(y|x) + (1 - \lambda) * p_{LM}(y|x)$$

### Implementation

í•œ ê°€ì§€ ë¬¸ì œì ì€, `Datastore` ì´ `billion` ê°œì˜ ë°ì´í„°ë¥¼ ë‚´í¬í•˜ê³  ìˆì–´ `computationally intensive` í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ `FAISS`ë¼ëŠ” `open source library` ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ì°¨ì› ìƒì—ì„œì˜ kNNì„ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ê²Œ ëœë‹¤. ì¶”ê°€ì ìœ¼ë¡œ, $L^2$ ì´ì™¸ì— `inner product distance` ë¼ëŠ” `distance function` ë˜í•œ ì¡´ì¬í•˜ëŠ”ë°, ì´ ëª¨ë¸ì—ì„œëŠ” $L^2$ ë°©ì‹ì´ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

### Related Cache Models

ì´ì „ì˜ ë¹„ìŠ·í•œ ì ‘ê·¼ ë°©ì‹ì—ì„œëŠ”, recent pastì— ëŒ€í•œ `caching` ì„ í†µí•´ì„œ ìµœê·¼ì˜ ë°ì´í„°ì— ëŒ€í•´ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ë°©ì‹ ë˜í•œ ì¡´ì¬í–ˆë‹¤. í•˜ì§€ë§Œ ìµœê·¼ì˜ ì •ë³´ë¥¼ copyí•  ìˆ˜ ìˆëŠ” `self-attention` ê¸°ë²•ì„ ê°€ì§„ `Transformer` ëª¨ë¸ì´ ë“±ì¥í•˜ê³  ë‚˜ì„œ, ì´ ë°©ì‹ì€ ì¸ê¸°ë¥¼ ìƒê²Œ ë˜ë©°, ì–»ì„ ìˆ˜ ìˆëŠ” ì´ìµ ë˜í•œ ì¤„ì–´ë“¤ì—ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” `training data`ì— ëŒ€í•œ ëª…ì‹œì  ê¸°ì–µì„ ìœ„í•´ ì˜¤ë¡œì§€ `training data`ì— ëŒ€í•´ì„œë§Œ `caching`í•˜ëŠ” ë°©ì‹ì„ íƒí•˜ì—¬ ë¹„ìŠ·í•œ casesì— ëŒ€í•œ íš¨ìœ¨ì„ ì¦ëŒ€ì‹œì¼°ë‹¤.

## 3. Experimental Setup ğŸ¥½

### Data

ë‹¨ìˆœíˆ datasetì— ëŒ€í•œ ì„¤ëª…ì´ê¸°ì— ë³„ë„ì˜ ì„¤ëª…ì€ ìƒëµí•œë‹¤.

![image](https://github.com/user-attachments/assets/61a7608f-8383-4799-b01c-fdd0bd523ff9)

### Model Architecture

*k*NN-LM ëª¨ë¸ì€ `fixed-size context representations`ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸ì´ë¼ë©´ ëª¨ë‘ í˜¸í™˜ì´ ê°€ëŠ¥í•˜ë‹¤. ì´ ëª¨ë¸ì€ í˜„ì¬ (ë‹¹ì‹œ) SOTAë¥¼ ê¸°ë¡í–ˆë˜ `Decoder-only Transformer`ì„ ì‚¬ìš©í•œë‹¤. *k*NN-LM ëª¨ë¸ì€ ëª¨ë¸ì˜ ê¸°ë³¸ ëª¨ë¸ì¸ LMì— ëŒ€í•œ í›ˆë ¨ì„ ì‹œí–‰í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ê¸°ì¡´ ì•„í‚¤í…ì³ì™€ ìµœì í™” ë°©ì‹ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í–ˆë‹¤.

`16 layer`, `each with 16 self-attention heads`, `1024 dimensional hidden states`, `4096 dimensional feedforward layers`, `247M parameters` ë“±ì„ ì‚¬ìš©í•˜ë©° ì¶”ê°€ ì •ë³´ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

![image](https://github.com/user-attachments/assets/0389fbaa-e033-4f79-9977-b2956de075fd)

### Evaluation

ì´ LMë“¤ì€ `negative log-likelihood` ë¥¼ `loss function`ìœ¼ë¡œ ì‚¬ìš©í–ˆìœ¼ë©°, ëª¨ë¸ì˜ í‰ê°€ ê¸°ì¤€ìœ¼ë¡œì¨ `perplexity`ë¥¼ ì‚¬ìš©í–ˆë‹¤. (_ì‚´ì§ ì²¨ì–¸í•˜ìë©´, ìµœê·¼ ë…¼ë¬¸ì¸ Mambaì˜ reject ì›ì¸ìœ¼ë¡œ `perplexity`ê°€ í‰ê°€ ê¸°ì¤€ì´ ë˜ëŠ” ê²ƒì€ ì •ë‹¹í•˜ì§€ ì•Šë‹¤ëŠ” ê¸€ì„ ë³¸ ê²ƒ ê°™ì€ë°, ì´ë•ŒëŠ” ê¸°ì¤€ì´ ì¡°ê¸ˆ ë‹¬ëë‚˜ë³´ë‹¤._)

### *k*NN-LM

![image](https://github.com/user-attachments/assets/a02f1907-bd0f-4026-b1ce-4dda9f8de28c)
![image](https://github.com/user-attachments/assets/0dd7ddd7-f4f6-4438-9003-a4f38935aeb5)

### Computational Cost

ì¶”ê°€ì ì¸ Training ì—†ì´ë„ ëª¨ë¸ì„ êµ¬í˜„í•  ìˆ˜ ìˆì§€ë§Œ, `key-value` í˜•íƒœì˜ `datastore` ì„ ìƒì„±í•˜ê¸° ìœ„í•˜ì—¬ 1 epoch ì •ë„ì˜ ì‹œê°„ì´ ì†Œìš”ëœë‹¤. ë˜í•œ keyë“¤ì´ ì €ì¥ëœ í›„ `WIKI-103M`ì˜ ìºì‹œë¥¼ CPUì— ì ìš©í•˜ëŠ” ë°ì— 2ì‹œê°„ì´ ì†Œìš”ë˜ë©°, 1024ê°œì˜ NNì„ êµ¬í•˜ëŠ” ë°ì— 25ë¶„ ì •ë„ê°€ ì†Œìš”ëœë‹¤. ë¬¼ë¡  ë°ì´í„°ì…‹ì˜ í¬ê¸°ì— ì„ í˜•ì ìœ¼ë¡œ ë¹„ë¡€í•˜ì—¬ ì‹œê°„ì´ ì¦ê°€í•˜ì§€ë§Œ, ì´ëŠ” ì‰½ê²Œ ë³‘ë ¬í™”ê°€ ê°€ëŠ¥í•˜ë©°, GPUì˜ ì‚¬ìš©ì´ í•„ìš”í•˜ì§€ ì•Šë‹¤.

## 4. Experiemtents

### 4.1 Using the Training data as the Datastore

![image](https://github.com/user-attachments/assets/eaf67693-4e89-4140-8b93-0fce95756113)

ê¸°ì¡´ì˜ SOTA ë°©ì‹ê³¼ ë¹„êµí•˜ì—¬, ë³¸ ë…¼ë¬¸ì˜ *k*NN-LM ì´ ì–¼ë§ˆë‚˜ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ëŠ”ì§€ë¥¼ ì‹¤í—˜í•´ë³´ì•˜ë‹¤. ì—¬ê¸°ì„œ Training data ê·¸ëŒ€ë¡œ `Datastore`ì— ëŒ€ì…í–ˆë‹¤. ê¸°ì¡´ì˜ SOTAì™€ ë¹„êµí•˜ì—¬ 18.65 ì—ì„œ 16.12ë¡œ ìƒˆë¡œìš´ SOTAë¥¼ ë‹¬ì„±í–ˆë‹¤.

ì¶”ê°€ì ìœ¼ë¡œ, `WIKI`ê°€ cachingì— ìœ ë… ì¢‹ì€ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ `BOOKS corpus`ë¥¼ ì´ìš©í•˜ì—¬ ê°™ì€ ì‹¤í—˜ì„ ë°˜ë³µí•´ ë³´ì•˜ë‹¤. ê·¸ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

![image](https://github.com/user-attachments/assets/787c6775-fb90-4bc9-9d49-c0015b8de7f7)

### 4.2 More data without Training

ì´ë²ˆì—ëŠ” `Training dataset`ê³¼ `Datastore`ì„ ë¶„ë¦¬í•˜ì—¬, ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì„ ì´ìš©í–ˆì„ ë•Œì—ë„ íš¨ê³¼ê°€ ìˆëŠ”ì§€ë¥¼ í™•ì¸í•´ë³´ì•˜ë‹¤.

![image](https://github.com/user-attachments/assets/dbbe9090-b6bb-4329-8471-2f6e618de7bd)

ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´, `WIKI-3B`ì™€ `WIKI-100M` datasetìœ¼ë¡œ ì‹¤í—˜í•´ ë³´ì•˜ëŠ”ë°, ë‹¹ì—°í•˜ê²Œë„ ë” í° ë°ì´í„°ì…‹ì¸ `WIKI-3B` ì„ ì´ìš©í•´ í•™ìŠµí–ˆì„ ë•Œ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. í•˜ì§€ë§Œ, `WIKI-100M`ìœ¼ë¡œ í•™ìŠµí•œ ë’¤ì— `WIKI-3B`ë¥¼ `datastore`ë¡œ í™œìš©í–ˆì„ ë•Œ, ê·¸ ì„±ëŠ¥ì´ ê¸°ì¡´ì˜ LMì„ ëŠ¥ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì•„, *k*NN-LM ë°©ì‹ì´ ë”ìš± íš¨ìœ¨ì ì´ê³  ì •í™•ë„ê°€ ë†’ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤.

![image](https://github.com/user-attachments/assets/42927bfe-fbdc-4deb-9873-04e58b963539)

ë˜í•œ, ìœ„ì™€ ê°™ì´ *k*NN-LMì˜ `datastore` í¬ê¸°ì— ëŒ€í•´ì„œë„ ì‹¤í—˜ì„ í•´ ë³´ì•˜ëŠ”ë°, 1.6Bì˜ datasetë§Œ ì‚¬ìš©í–ˆì„ ë•Œ ì´ë¯¸ Vanilla LMì˜ ì„±ëŠ¥ì„ ëŠ¥ê°€í•˜ì˜€ê³ , 3Bì˜ ê²½ìš°ì—ë„ ê·¸ëŸ¬í–ˆë‹¤. ë” ë‚˜ì•„ê°€ 3Bì˜ ê²½ìš°ì—ë„ `perplexity`ì˜ ê°ì†Œë„ê°€ `saturated`(í¬í™”) ë˜ì§€ ì•ŠëŠ” ê²ƒì„ ë³´ì•„, ë” í° ì ì¬ì„±ì´ ì¡´ì¬í•œë‹¤.

ë§ˆì°¬ê°€ì§€ë¡œ, ê° `datastore` í¬ê¸°ì— ëŒ€í•´ì„œ `optimal` í•œ $\lambda$ë¥¼ êµ¬í•´ ë³´ì•˜ì„ ë•Œ, ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ ê²°ê³¼ê°€ ë‚˜íƒ€ë‚¬ë‹¤.

### 4.3 Domain Adaptation

`Domain Adaptation` ì‹¤í—˜ì„ ìœ„í•˜ì—¬, `WIKI-3B`ë¡œ í•™ìŠµëœ ëª¨ë¸ì„ BOOK ì„ datasetìœ¼ë¡œ inference í•´ë³´ì•˜ë‹¤. ê·¸ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

![image](https://github.com/user-attachments/assets/a6de92b7-7688-4af3-a870-c6204ba8a9fe)

ìˆœìˆ˜í•˜ê²Œ `datastore` ì—†ì´ ì¶”ë¡ í•œ ê²°ê³¼ ë§¤ìš° ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì˜€ì§€ë§Œ, `BOOKS` ë¥¼ `datastore` ë¡œ í™œìš©í•˜ê²Œ ë˜ë©´, perplexityê°€ 15 ê°€ê¹Œì´ ë–¨ì–´ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì¦‰ target domainì— ëŒ€í•œ `datastore`ì´ ìˆë‹¤ë©´, ì¶©ë¶„íˆ ë‹¤ë¥¸ domainìœ¼ë¡œ ì ìš©ì´ ê°€ëŠ¥í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.

## 5. Tuninig Nearest Neighbor Search

### Key Function

`Similarity Search` ë¥¼ ìœ„í•˜ì—¬, `prior context`ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ `fixed-size representation` ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” $f()$ëŠ” ì¤‘ìš”í•œ ìš”ì†Œì´ë‹¤. ì´ë¥¼ ì‹¤í—˜í•´ë³´ê¸° ìœ„í•´, Transformer êµ¬ì¡°ì˜ ë‹¤ì–‘í•œ ë¶€ë¶„ì„ í›„ë³´ë¡œ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ë‹¤. (ëª¨ë‘ Transformerì˜ ë§ˆì§€ë§‰ layer ë¶€ë¶„ì´ë‹¤.)

![image](https://github.com/user-attachments/assets/ccb71055-4a40-44ec-8ed6-51a090d938e5)

ì‹¤í—˜ì—ì„œ ë³´ëŠ” ê²ƒê³¼ ê°™ì´, `FFN input after layer norm` ë¶€ë¶„ì´ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ì—ˆê³ , ì¶”ê°€ì ìœ¼ë¡œ ë§ˆì§€ë§‰ ì§ì „ì˜ layer (second-last) ì—ë„ ì‹¤í—˜ì„ í•´ë³´ì•˜ìœ¼ë‚˜, ë¹„ìŠ·í•œ ê²½í–¥ì˜ ì ìˆ˜ì§€ë§Œ ì‚´ì§ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

ì´ë¥¼ í†µí•´ FFNì€ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì—, MHSAëŠ” representation ì— ë”ìš± íš¨ê³¼ì ì¸ ê²ƒì´ë¼ê³  ì¶”ì¸¡í•´ë³¼ ìˆ˜ ìˆë‹¤.

### Other elements (Number of Neighbors, Interpolation Param, etc)

![image](https://github.com/user-attachments/assets/2a327ff6-8340-499d-9989-650c211cd090)

ê·¸ë¦¼ì—ì„œ ë³´ë‹¤ì‹œí”¼, k-NN ì—ì„œì˜ k ë¥¼ í•˜ë‚˜ì˜ ìš”ì†Œë¡œ, `interpolation` ë³€ìˆ˜ë¥¼ í•˜ë‚˜ì˜ ìš”ì†Œë¡œ í•˜ì—¬ ì‹¤í—˜ì„ ì§„í–‰í–ˆë‹¤. ê·¸ ê²°ê³¼ k ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ `perplexity`ê°€ ê°ì†Œí•˜ë©°, ê° ìƒí™© (`In-domain or Domain Adaptation`)ì— ë§ì¶”ì–´ ì ì ˆí•¨ $\lambda$ ê°’ì´ í˜•ì„±ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

ë˜í•œ, `Similarity Function`ì˜ `precision`ì— ëŒ€í•´ì„œë„ ì‹¤í—˜ì„ ì§„í–‰í–ˆëŠ”ë°, $L^2$ distanceì— ëŒ€í•´ì„œ `full precision`ì„ í†µí•´ ì—°ì‚°í•¨ìœ¼ë¡œì¨ perplexity ê°€ 16.5ì—ì„œ 16.06ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

## 6. Analysis

### Qualitative Analysis

$p_{kNN}$ ì´ ì™œ $p_{LM}$ ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ê°€ì§€ëŠ”ì§€ ì´í•´í•˜ê¸° ìœ„í•˜ì—¬ $p_{kNN}$ ì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë˜ ì˜ˆì‹œë“¤ì„ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

![image](https://github.com/user-attachments/assets/2cb08fc1-fc04-4d35-9af2-f58cf5088205)

ì˜ˆì‹œì—ì„œ ë³´ë‹¤ì‹œí”¼, *k*NN-LM modelì€ `rare patterns` ì— ëŒ€í•´ì„œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ì´ëŠ” ê³§ `factual knowledge`ë¥¼ ëœ»í•œë‹¤. íŠ¹íˆ training setì— ì¡´ì¬í•˜ê±°ë‚˜ ë¹„ìŠ·í•œ `context`ì˜ ê²½ìš° ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

ì´ë¥¼ í†µí•´ parametersë¥¼ í†µí•´ ì§€ì‹ì„ `implicit` í•˜ê²Œ í•™ìŠµí•˜ëŠ” ê²ƒë³´ë‹¤ `explicit` í•˜ê²Œ, ì¦‰ `Nearest Neighbor`ì„ ì°¾ì•„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì´ ë”ìš± íš¨ìœ¨ì ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.

### Simple vs Neural Representation

[![image](https://github.com/user-attachments/assets/6b79e842-53e6-4a1b-84d8-5730e94d2070)](#fig-78)

í•˜ì§€ë§Œ, ì´ëŸ° `rare patterns` (long-tail phenomena)ëŠ” ë‹¨ìˆœíˆ `N-gram model`ì—ì„œë„ ì¶©ë¶„íˆ ì„±ëŠ¥ì„ ê°€ì§ˆ ìˆ˜ë„ ìˆë‹¤. ë”°ë¼ì„œ `n-gram` ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì‹œì¼œë³´ì•˜ëŠ”ë°, ê²°ê³¼ëŠ” ìœ„ì™€ ê°™ì´ `n-gram model`ì€ í˜„ì €íˆ ë‚®ì€ ì •í™•ë„ë¥¼ ë‚˜íƒ€ë‚´ì—ˆë‹¤. ì´ë¥¼ ë¯¸ë£¨ì–´ ë³´ë©´, *k*NN-LMì€ ë‹¨ìˆœíˆ `local contextë¥¼` í•™ìŠµí•˜ëŠ” ê²ƒ ì´ìƒìœ¼ë¡œ, `global context`ë¥¼ í•™ìŠµí•œë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.

### Implicit vs Explicit Memory

ê·¸ë ‡ë‹¤ë©´ ê³¼ì—° Explicit Memory ê°€ Implicit Memory ë³´ë‹¤ íš¨ê³¼ì ì¼ê¹Œ? ë³¸ ë…¼ë¬¸ì—ì„œì˜ datastore ì„ LM ì´ ëª¨ë‘ ì™¸ìš°ëŠ” ê²ƒì´ ê°€ëŠ¥í• ê¹Œ?

ì´ë¥¼ ì‹¤í—˜í•´ë³´ê¸° ìœ„í•˜ì—¬, Transformer ì„ `dropout` ì—†ì´ í•™ìŠµì‹œì¼œ, `datastore` ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ í•™ìŠµí•˜ë„ë¡ í–ˆë‹¤. ê·¸ ê²°ê³¼ ì•ì„  [ê·¸ë¦¼](#fig-78)ê³¼ ê°™ì´, loss ê°€ ì•„ì˜ˆ 0ìœ¼ë¡œ ë–¨ì–´ì§„ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆê³ , ì´ëŠ” ê³§ `datastore` ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì •í™•í•˜ê²Œ í•™ìŠµí–ˆë‹¤ëŠ” ì˜ë¯¸ê°€ ëœë‹¤.

ë”°ë¼ì„œ ì´ë ‡ê²Œ `overfitting` ëœ LMê³¼ `explicit memory`ì¸ `datastore`ë¥¼ ë¹„êµí•˜ê¸° ìœ„í•˜ì—¬ ê°ê°ì„ original LM ì— `interpolate` í•˜ì—¬ `perplexity`ë¥¼ ì¸¡ì •í–ˆëŠ”ë°, LMì˜ ê²°ê³¼ 0.1 í–¥ìƒ, `datastore` ì˜ ê²°ê³¼ 1.9 í–¥ìƒìœ¼ë¡œ `explicit memory`ê°€ ë” ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

ì´ ì‹¤í—˜ì˜ ê²°ê³¼ë¡œ, Transformer LMì€ datastoreì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì™¸ìš¸ ì •ë„ë¡œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ì§€ë§Œ, ì´ ê²½ìš° generalize ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤ëŠ” ê²ƒì„ ì¶”ì¸¡í•´ë³¼ ìˆ˜ ìˆë‹¤.

## 8. Conclusion & Future Work

> Related Work Section ì€ ì¤‘ìš”í•œ ë¶€ë¶„ì´ ì•„ë‹ˆë¼ê³  íŒë‹¨ë˜ì–´ ì œì™¸í•˜ì˜€ìŠµë‹ˆë‹¤.

ì´ë ‡ë“¯ *k*NN-LM modelì€ ê¸°ì¡´ì˜ standard LMì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. ì´ëŸ° ì ‘ê·¼ ë°©ì‹ì€ ì„ì˜ì˜ `NLP task` ì— ì ìš©ë  ìˆ˜ ìˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì˜ ì„±ê³µì€ ì¸í•˜ì—¬ `context` ê°„ì˜ `similarity` ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ë‹¤ìŒ í† í°ì„ í•™ìŠµí•˜ëŠ” ê²ƒë³´ë‹¤ ì‰¬ìš´ taskì„ì„ ëœ»í•˜ê¸°ë„ í•œë‹¤. ì¶”í›„ì—ëŠ” ì´ëŸ° `simlarity function` ì„ ëª…ì‹œì ìœ¼ë¡œ í•™ìŠµí•˜ê±°ë‚˜, `datastore`ì˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ê²ƒ ë“±ì˜ ì—°êµ¬ê°€ í•„ìš”í•  ê²ƒì´ë‹¤.
