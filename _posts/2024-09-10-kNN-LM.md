---
title: "[Paper Review] Generalization Through Memorization: Nearest Neighbor Language Models"
description: "Paper Review for kNN-LM model"
writer: Sangyun Won
categories: [AI, Paper Review]
tags: [AI]
image:
  path: https://github.com/user-attachments/assets/f44a25a1-dc34-46d6-b024-8a1a881f5953
  alt: Paper Review for kNN-LM model

math: true
toc: true
toc_sticky: true

date: 2024-09-10
last_modified_at: 2024-09-10
---

<style>
  figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}
.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}
</style>

## 0. Abstract ğŸ¬

`kNN-LM` ì€ `pre-train` ëœ LMì„ ì„ í˜•ì ìœ¼ë¡œ kNN ì•Œê³ ë¦¬ì¦˜ê³¼ ê²°í•©í•˜ì—¬ í™•ì¥í•œ ëª¨ë¸ì´ë‹¤. `pre-trained LM`ì„ ì´ìš©í•´ì„œ input ë°ì´í„°ê°€ `latent space` ë¡œ ì„ë² ë”© ë˜ê²Œ ë˜ëŠ”ë°, ì´ `latent space` ìƒì˜ ë²¡í„° ê°„ì˜ ê±°ë¦¬ë¥¼ í†µí•´ì„œ ê°€ì¥ ê°€ê¹Œìš´ k ê°œì˜ í›„ë³´ë¥¼ ì •í•˜ê²Œ ëœë‹¤. ì´ëŠ” ì„ì˜ì˜ ë°ì´í„°ì…‹ (including training data) ì„ í†µí•´ì„œ ê°€ëŠ¥í•˜ë‹¤.

ì´ ë°©ì‹ì„ `Wikitext-103LM` ì— ì ìš©í•¨ìœ¼ë¡œì¨, ì´ ë…¼ë¬¸ì—ì„œ ì†Œê°œí•˜ëŠ” ëª¨ë¸ì€ SOTAë¥¼ ë‹¬ì„±í–ˆìœ¼ë©°, ì¶”ê°€ì ì¸ training ì—†ì´ë„ 15.79ì˜ `perplexity`ë¡œ 2.9 pointë‚˜ ì¤„ì´ëŠ” íš¨ê³¼ë¥¼ ë³´ì˜€ë‹¤. ë˜í•œ, ì´ ì ‘ê·¼ë²•ì€ ë” í° í›ˆë ¨ ë°ì´í„°ì…‹, ê·¸ë¦¬ê³  ë‹¤ë¥¸ `domain` ìœ¼ë¡œì˜ ì ìš© ì—­ì‹œ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆì—ˆë‹¤.

ì§ˆì ìœ¼ë¡œëŠ”, ì´ ëª¨ë¸ì€ ìƒì†Œí•œ í‘œí˜„ë“¤ì— ëŒ€í•´ì„œ ë”ìš± íš¨ê³¼ì ì¸ ëª¨ìŠµì„ ë³´ì˜€ê³ , íŠ¹íˆ `factual knowledege` ì— ëŒ€í•´ì„œ íš¨ê³¼ì ì´ì˜€ë‹¤. ë™ì‹œì— ì´ ì—°êµ¬ëŠ” LM ì˜ ê·¼ë³¸ì ì¸ task ì¸ `next token prediction` ë³´ë‹¤ `sequences ê°„ì˜ similarity` ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ë” íš¨ê³¼ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ì„ ì˜ë¯¸í•˜ê¸°ë„ í•œë‹¤.

## 1. Introduction â˜•ï¸

Language Model ì€ ì¼ë°˜ì ìœ¼ë¡œ ì•„ë˜ì˜ 2ê°€ì§€ taskë¥¼ ëª©í‘œë¡œ í•œë‹¤.

**1. ì¦‰ ë¬¸ì¥ì˜ prefixë¥¼ nì°¨ì› ë²¡í„°ë¡œ ë‚˜íƒ€ë‚¸ë‹¤. (ì •í™•íˆëŠ” ê³ ì • í¬ê¸°ì˜ representation)**

**2. ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ latent spaceì—ì„œì˜ ê°’ì„ ì´ìš©í•´ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•œë‹¤.**

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì²«ë²ˆì§¸ taskê°€ ë‘ë²ˆì§¸ task ë³´ë‹¤ ì‰¬ìš´ taskë¼ëŠ” ê°€ì • í•˜ì— ì ‘ê·¼í–ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, "_Dickens is the author of_" ë¼ëŠ” ë¬¸ì¥ê³¼ "_Dickens wrote_" ë¼ëŠ” ë¬¸ì¥ì„ ë³´ì•˜ì„ ë•Œ, ê·¸ í›„ì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ì§€ ëª»í•˜ë”ë¼ë„ ë‘ ë¬¸ì¥ì´ ê°™ì€ ëœ»ì„ ë‚´í¬í•˜ê³  ìˆìŒì€ ëˆ„êµ¬ë‚˜ ì•Œ ìˆ˜ ìˆë‹¤. ì‹¤í—˜ì ìœ¼ë¡œë„, `prefix embedding`ì— ëŒ€í•´ kNNì„ ì ìš©ì‹œí‚¨ ê²°ê³¼, ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ í†µí•´ LMì´ ì²«ë²ˆì§¸ taskì— ë” íš¨ê³¼ì ì´ë¼ëŠ” ê°•ë ¥í•œ ì¦ê±°ë¥¼ ì œì‹œí•œë‹¤.

`3-billion`ê°œì˜ `token`ì„ ëª¨ë¸ì˜ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤, `100-million`ê°œì˜ `token`ì„ ì´ìš©í•´ í•™ìŠµí•˜ê³  3-billionê°œì˜ `token`ì„ ê°€ì§€ëŠ” `dataset(documents)`ì„ ì´ ëª¨ë¸ì— ì ìš©í•˜ëŠ” ê²ƒì´ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. ì´ëŠ” ê³§ Large datasetì„ ì‚¬ìš©í•˜ëŠ” LMì— ëŒ€í•œ ìƒˆë¡œìš´ ë°©í–¥ì„±ì„ ì œì‹œí•œë‹¤. ë¹„ìŠ·í•˜ê²Œ, ë‹¨ìˆœíˆ `datastore` ì— ë‹¤ë¥¸ domainì˜ ë°ì´í„°ë¥¼ ì‚½ì…í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ `multiple domain`ì—ì„œë„ íš¨ê³¼ì ì¸ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

![image](https://github.com/user-attachments/assets/f44a25a1-dc34-46d6-b024-8a1a881f5953)

ë§ˆì§€ë§‰ìœ¼ë¡œ, ì´ ëª¨ë¸ì€ ëª…ì‹œì  ê¸°ì–µì— ëŒ€í•œ ì ‘ê·¼ (datastoreì´ë¼ëŠ” ëª…ì‹œì ì¸ ë°ì´í„°) ì„ í†µí•´ `long-tail patterns` (ì˜ˆë¥¼ ë“¤ì–´ `Factual Knowledge`) ì— ëŒ€í•´ì„œ ë”ìš± íš¨ê³¼ì ì¸ ê²ƒì„ ë°œê²¬í–ˆë‹¤.

## 2. Nearest Neighbor Language Modeling ğŸ§

LMì€ ê¸°ë³¸ì ìœ¼ë¡œ `sequence`ì— ëŒ€í•œ í™•ë¥ ì„ í• ë‹¹í•œë‹¤. ë‹¤ì‹œ ë§í•´ $c_t = (w_1, \cdots , w_{t-1})$ ë¼ëŠ” `context (sequence)`ê°€ ì£¼ì–´ì ¸ ìˆì„ ë•Œ, LM (autoregressive í•œ)ì€
$p(w_t|c_t)$ë¥¼ ê³„ì‚°í•´ë‚¸ë‹¤.

*k*NN-LMì€ `pre-trained LM`ì„ ì´ìš©í•˜ì—¬ `nearest-neighbors`ë¥¼ ê²€ìƒ‰(`retrieval`) í•˜ì—¬ `augument` í•˜ëŠ” ê³¼ì •ì„ ë‚´í¬í•˜ë©°, ì´ datastoreì—ëŠ” `key-value` í˜•íƒœì˜ `context-target` ìŒë“¤ì´ ì €ì¥ë˜ì–´ ìˆë‹¤. (see Figure 1)

### Datastore

$f()$ë¥¼ context $c$ë¥¼ `fixed-length vector`ë¡œ `mapping`í•´ì£¼ëŠ” í•¨ìˆ˜ë¼ê³  ê°€ì •í•´ë³´ì. ë§Œì•½ì— ië²ˆì§¸ training dataì¸ $(c_i, w_i) \in \mathcal{D}$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ìš°ë¦¬ëŠ” `datastore`ì„ ë‹¤ìŒê³¼ ê°™ì€ `key-value` ì§‘í•©ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

$$
(\mathcal{K}, \mathcal{V}) = \{(f(c_i), w_i)|(c_i, w_i)\} \in \mathcal{D}
$$

### Inference

ì´ ëª¨ë¸ì€ `input context` $x$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, $f(x)$ë¥¼ í†µí•´ $p_{LM}(y|x)$ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ê³„ì‚°í•˜ê²Œ ëœë‹¤. ë™ì‹œì— datastore ì— $f(x)$ë¥¼ ì´ìš©í•´ queryë¥¼ ë³´ë‚´ê²Œ ë˜ëŠ”ë°, `distance function`ì¸ $d()$ë¥¼ í†µí•´ `k-nearest neighbors`ì— í•´ë‹¹í•˜ëŠ” ì§‘í•© $\mathcal{N}$ì„ ìƒì„±í•œë‹¤.
(ë³¸ ë…¼ë¬¸ì—ì„œëŠ” `distance function`ì„ $L^2$ distanceë¡œ ì •ì˜í–ˆë‹¤.)

ê·¸ í›„ì—ëŠ” `softmax`ì— `negative distance`ë¥¼ ë„£ìŒìœ¼ë¡œì¨, ì•„ë˜ì˜ í™•ë¥ ì„ ê³„ì‚°í•´ë‚¼ ìˆ˜ ìˆë‹¤. (ê±°ë¦¬ê°€ ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ í™•ë¥ ì˜ ì •í™•ë„ë¥¼ ë³´ì¸ë‹¤.)

$$
p*{kNN}(y|x) \propto \sum_{(k*i, v_i) \in \mathcal{N}}{\mathbb{1}_{y=v_i} \, exp(-d(k_i, f(x)))}
$$

ì´ë¥¼ ì„ í˜•ì ìœ¼ë¡œ ê¸°ì¡´ LMì— ì ìš©í•˜ê²Œ ë˜ë©´, $\lambda$ ë³€ìˆ˜ë¥¼ ì´ìš©í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ìµœì¢… `probability`ë¥¼ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

$$p(y|x) = \lambda * p_{kNN}(y|x) + (1 - \lambda) * p_{LM}(y|x)$$

### Implementation

í•œ ê°€ì§€ ë¬¸ì œì ì€, `Datastore` ì´ `billion` ê°œì˜ ë°ì´í„°ë¥¼ ë‚´í¬í•˜ê³  ìˆì–´ `computationally intensive` í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ `FAISS`ë¼ëŠ” `open source library` ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ì°¨ì› ìƒì—ì„œì˜ kNNì„ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ê²Œ ëœë‹¤. ì¶”ê°€ì ìœ¼ë¡œ, $L^2$ ì´ì™¸ì— `inner product distance` ë¼ëŠ” `distance function` ë˜í•œ ì¡´ì¬í•˜ëŠ”ë°, ì´ ëª¨ë¸ì—ì„œëŠ” $L^2$ ë°©ì‹ì´ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

### Related Cache Models

ì´ì „ì˜ ë¹„ìŠ·í•œ ì ‘ê·¼ ë°©ì‹ì—ì„œëŠ”, recent pastì— ëŒ€í•œ `caching` ì„ í†µí•´ì„œ ìµœê·¼ì˜ ë°ì´í„°ì— ëŒ€í•´ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ë°©ì‹ ë˜í•œ ì¡´ì¬í–ˆë‹¤. í•˜ì§€ë§Œ ìµœê·¼ì˜ ì •ë³´ë¥¼ copyí•  ìˆ˜ ìˆëŠ” `self-attention` ê¸°ë²•ì„ ê°€ì§„ `Transformer` ëª¨ë¸ì´ ë“±ì¥í•˜ê³  ë‚˜ì„œ, ì´ ë°©ì‹ì€ ì¸ê¸°ë¥¼ ìƒê²Œ ë˜ë©°, ì–»ì„ ìˆ˜ ìˆëŠ” ì´ìµ ë˜í•œ ì¤„ì–´ë“¤ì—ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” `training data`ì— ëŒ€í•œ ëª…ì‹œì  ê¸°ì–µì„ ìœ„í•´ ì˜¤ë¡œì§€ `training data`ì— ëŒ€í•´ì„œë§Œ `caching`í•˜ëŠ” ë°©ì‹ì„ íƒí•˜ì—¬ ë¹„ìŠ·í•œ casesì— ëŒ€í•œ íš¨ìœ¨ì„ ì¦ëŒ€ì‹œì¼°ë‹¤.

## 3. Experimental Setup ğŸ¥½

### Data

ë‹¨ìˆœíˆ datasetì— ëŒ€í•œ ì„¤ëª…ì´ê¸°ì— ë³„ë„ì˜ ì„¤ëª…ì€ ìƒëµí•œë‹¤.

![image](https://github.com/user-attachments/assets/61a7608f-8383-4799-b01c-fdd0bd523ff9)

### Model Architecture

*k*NN-LM ëª¨ë¸ì€ `fixed-size context representations`ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸ì´ë¼ë©´ ëª¨ë‘ í˜¸í™˜ì´ ê°€ëŠ¥í•˜ë‹¤. ì´ ëª¨ë¸ì€ í˜„ì¬ (ë‹¹ì‹œ) SOTAë¥¼ ê¸°ë¡í–ˆë˜ `Decoder-only Transformer`ì„ ì‚¬ìš©í•œë‹¤. *k*NN-LM ëª¨ë¸ì€ ëª¨ë¸ì˜ ê¸°ë³¸ ëª¨ë¸ì¸ LMì— ëŒ€í•œ í›ˆë ¨ì„ ì‹œí–‰í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ê¸°ì¡´ ì•„í‚¤í…ì³ì™€ ìµœì í™” ë°©ì‹ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í–ˆë‹¤.

`16 layer`, `each with 16 self-attention heads`, `1024 dimensional hidden states`, `4096 dimensional feedforward layers`, `247M parameters` ë“±ì„ ì‚¬ìš©í•˜ë©° ì¶”ê°€ ì •ë³´ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

![image](https://github.com/user-attachments/assets/0389fbaa-e033-4f79-9977-b2956de075fd)

### Evaluation

ì´ LMë“¤ì€ `negative log-likelihood` ë¥¼ `loss function`ìœ¼ë¡œ ì‚¬ìš©í–ˆìœ¼ë©°, ëª¨ë¸ì˜ í‰ê°€ ê¸°ì¤€ìœ¼ë¡œì¨ `perplexity`ë¥¼ ì‚¬ìš©í–ˆë‹¤. (_ì‚´ì§ ì²¨ì–¸í•˜ìë©´, ìµœê·¼ ë…¼ë¬¸ì¸ Mambaì˜ reject ì›ì¸ìœ¼ë¡œ `perplexity`ê°€ í‰ê°€ ê¸°ì¤€ì´ ë˜ëŠ” ê²ƒì€ ì •ë‹¹í•˜ì§€ ì•Šë‹¤ëŠ” ê¸€ì„ ë³¸ ê²ƒ ê°™ì€ë°, ì´ë•ŒëŠ” ê¸°ì¤€ì´ ì¡°ê¸ˆ ë‹¬ëë‚˜ë³´ë‹¤._)

### *k*NN-LM

<figure id="8b48f93f-7612-4d74-8935-a028926a8762">
  <a href="https://facerain.github.io/nlp-tutorial-roadmap/" class="bookmark source">
    <div class="bookmark-info">
      <div class="bookmark-text">
        <div class="bookmark-title">[5ë¶„ NLP] NLP ê³µë¶€ ì‹œì‘í•˜ê¸°</div>
        <div class="bookmark-description">NLP ê³µë¶€ ì–´ë–»ê²Œ ì‹œì‘í•˜ë©´ ì¢‹ì„ê¹Œìš”?</div>
      </div>
    </div>
    <img src="https://facerain.github.io/static/e14d9a9714a527a3420c0312b381ee9f/th.jpg" class="bookmark-image" alt="img"/>
  </a>
</figure>
